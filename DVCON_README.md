# DVCon PDF Abstracts

This file is generated from DVCon PDFs using a simple OCR-capable 
pipeline. For best results, install the optional dependencies:

```bash
pip install pypdf pdf2image pytesseract
```

You may also need to install system packages for Poppler and 
Tesseract OCR (see the respective project documentation).

The canonical project abstract and search-friendly description are 
maintained in the main `README.md`. This file is intended as a 
supporting appendix that lists per-paper abstracts extracted from 
DVCon PDFs.

| Paper | Abstract |
| --- | --- |
| 1A1 DVConIndia2025 Final Paper 4877 | Complex System-on-Chip (SoC) designs extensively rely on intricate interrupt mechanisms for event handling. Traditional hardware interrupt verification uses unstandardized, manual, and error-prone methods that are difficult to scale or reuse, especially dealing with complex interrupt aggregation points. This paper introduces a novel, reusable UVM-based framework designed to overcome the aforementioned issues. We present a solution that models interrupts as a hierarchical object tree, featuring a central manager for building these hierarchies and an intelligent monitor that automates runtime source identification, reporting and end-of-simulation checks. The architecture’s key innovation is its clear separation of DUT-specific integration from the generic, scalable, reusable logic. We detail the design, demonstrate its application on a complex Multimedia Processing Unit (MPU) and discuss its benefits for achieving scalable and robust interrupt verification. |
| 1A1 DVConIndia25 Final PPT 4877 | N/A |
| 1A2 DVConIndia2025 Final Paper 7616 | - As modern SoCs continue to grow in complexity, debugging remains one of the most time-consuming and resource-intensive phases in frontend verification, both at the IP and SoC levels. In large SoC projects,design blocks are a mix of new, reused, and slightly modified IPs. A common challenge arises when DV engineers, particularly those new to a reused block, encounter test failures and must rely on colleagues with prior experience for debug guidance. This dependency often leads to delays, especially when team members are unavailable or distributed across time zones. This paper presents a novel, experience-driven debug assistance tool designed to reduce this dependency and accelerate the debug process. The tool captures and stores error signatures observed, along with their most probable root causes, effectively mimicking the knowledge transfer that typically occurs between experienced and new verification engineers. When a test fails, the tool analyzes the failure pattern and provides likely debug pointers based on historical data, enabling engineers to resolve issues faster and more independently. I. |
| 1A2 DVConIndia25 Final PPT 7616 | N/A |
| 1A3 DVConIndia2025 Final Paper 5986 | - UVM Testbenches continue to be both commonly used and dreaded by verification engineers. Enabling more test creation and more test writers all using the standard UVM interfaces will allow an increase in productivity. This paper will extend previous work that presents a simple task based, C callable interface for tests. This paper will demonstrate an actual implementation of task-based tests built on commonly used bus protocols. Task based tests can be called from any compatible "C callable" interface system, including C, C++, SystemC, Python, Rust, PSS, etc. I. |
| 1A3 DVConIndia25 Final PPT 5986 | N/A |
| 1B1 DVConIndia2025 Final Paper 3272 | - UVM Virtual Sequences are used to coordinate st imulus activity across multiple design interfaces. The newest virtual sequence technique util izes a sequencer pool, which si mplifies and unifies the executi on of sequences and virtual sequences. The other three virtual sequence techniques described in this paper are (1) the Virtual Sequencers technique, (2) the test_base init_vseq Technique, and (3) storing and Retrieving Sequencer Handles in the UVM Resource Database. This paper summarizes and examin es: (1) the four virtual sequence techniques . (2) Why engineers sh ould use one of the sequencer aggregator techniques. (3) Why the most commonly used Virtual Sequencer technique is no longer recommended. I. |
| 1C1 DVConIndia2025 Final Paper 7562 | -The  verification  of  Central  Processing  Unit  (CPU)  accessible  registers  in  complex  System-on-Chips  (SoCs) presents a significant challenge, often caught between the powerful abstraction of the Universal Verification Methodology (UVM) Register Abstraction Layer (RAL) and the bare-metal necessity of C-based tests. C-based tests, essential for architectural and boot-path validation, traditionally lack the sophisticated modeling capabilities of UVM RAL, leading to inefficient, brittle tests and a high incidence of false failures from undocumented or complex register behaviors. This paper introduces a novel, automated methodology that systematically bridges this gap. We leverage UVM RAL as the reference source to generate portable, robust, and efficient C-compatible register test collaterals. The proposed flow utilizes a UVM sequence to extract comprehensive register metadata—including waivers and access policies—into an intermediate Comma-Separated Values (CSV) format, which is then synthesized into C data structures. Furthermore, we detail an intelligent register pruning technique that reduced test suite execution time by over 60% in a production environment. This methodology enhances verification accuracy by seamlessly porting IP-level waivers to the SoC context, has directly led to the identification of 35+ critical design defects, and has fundamentally improved regression efficiency and time-to-market. |
| 1C3 DVConIndia2025 Final Paper 2809 | -Clock Tree Network (CTN) is one of the complex design structures in SOC that controls clock supply for all the synchronized logic on the chip.  It comprises numerous clock component instances enabled with dynamic control to establish Dynamic Voltage Frequency Scaling (DVFS) and Hardware Controlled Auto Clock Gating (HWACG) schemes, effectively contributing to overall performance and power optimization [1].  Due to increasing complexity and stringent performance requirements, the verification of the CTN has also become more intricate.  This paper presents a robust verification strategy with Clock Monitor (CLKMON), a custom verification module developed to monitor, analyze and validate dynamic nature of CTN .  “Accelerated Clock Reference Model Generator” (ACRMG), an in -house tool developed to automate CTN analysis [2], is used to integrate  the CLKMON for all the IP clocks .  The paper also presents how CLKMON is integrated with various SOCs having different CTN topology. I. |
| 2A2 DVConIndia2025 Final Paper 4137 | - Single-precision fused multiply -add (FMA) units are fundamental building blocks in AI/ML accelerators, offering high arithmetic throughput and reduced rounding errors critical for training and inference workloads. Their increased adoption in performance -driven architecture demands rigorous and complete formal convergence to ensure functional correctness across all corner cases. However, verifying these designs poses significant challenges: FMAs feature deep arithmetic pipelines and extended exponent widths, both of which enlarge the symbolic state space and exacerbate corner-case sensitivity. Traditional verification strategies —such as assume -guarantee reasoning, helper assertions or case-splits often struggle in this context. These methods rely on clean modularity and local reasoning, which break down in FMAs due to tightly coupled Datapath stages, nonlinear interactions between operand paths, and fused rounding logic. This complexity leads to state -space explosion, fragile constraints, and frequent tool timeouts, making convergence infeasible through standard approaches. We demonstrate a targeted proof decomposition methodology that systematically partitions the algorithm into smaller, logically coherent verification units such as pre -alignment, multiplication, normalization, and rounding stages — based on their structural and arithmetic characteristics. A key aspect of our approach is constraint migration, which ensures that high -level design assumptions and properties are seamlessly transferred and preserved throughout the verification process at the RTL level, maintaining consistency and correctness across abstraction layers. By aligning verification sub-problems with the solver strengths of different tools and leveraging constraint migration, we achieved exhaustive convergence in 4 –6 weeks—cutting down the formal closure timeline by over 50% compared to traditional approaches that typically require 12 –14 weeks for similar complexity designs. Applied to an industrial -grade single-precision FMA design with extended exponent width, our methodology demonstrates practical scalability and complete end-to-end signoff viability in real-world Datapath verification scenarios. I. |
| 2B3 DVConIndia2025 Final Paper 5338 | - Formal verification is a pivotal methodology for ensuring the accuracy and dependability of complex digital designs. Its capacity to exhaustively and swiftly verify the design-under-test (DUT) offers a distinct advantage over conventional simulation-based methods. As intellectual property (IP) blocks grow in complexity, applying formal verification to these designs becomes increasingly challenging. A primary obstacle for formal verification engineers is achieving convergence, a difficulty that escalates with design intricacy. This paper examines the deployment of formal verification on a complex graphics block the compression controller. We elucidate our strategy, employing diverse abstraction techniques to manage complexity and enhance convergence. By systematically navigating the intricacies of compression logic, we demonstrate the efficacy of formal verification in ensuring design compliance and optimizing the verificati on process. This study highlights the critical role of formal verification in contemporary hardware development and offers insights into refining verification strategies for sophisticated systems. I. |
| 2B3 DVConIndia25 Final PPT 5338 | N/A |
| 2C2 DVConIndia2025 Final Paper 9003 | — Ensuring robust hardware security in increasingly complex and integrated System-on-Chip (SoC) designs require systematic, formal, and scalable methodologies. This paper presents a category-driven framework that unifies the Confidentiality-Integrity-Availability (CIA) triad with the rigor of formal verification, integrating both Security Path Verification (SPV) and Formal Property Verification (FPV). We detail a ten-category hardware security classification scheme aligned to Common Weakness Enumeration (CWE) and demonstrate how security properties are systematically derived from this taxonomy. By employing CWE-driven property generation, strategic abstraction, relaxed constraint handling, and staged convergence, our methodology delivers comprehensive, reusable, and early-stage security verification. This approach enables efficient identification and mitigation of vulnerabilities, not only for confidentiality, but equally for integrity and availability, shifting hardware security left in the design lifecycle. The presented framework is broadly reusable, extensible, and provides actionable pathways for rigorous hardware security analysis in modern SoC environments. |
| 3C1 DVConIndia2025 Final Paper 0076 | N/A |
| 3C3 DVConIndia2025 Final Paper 7251 | - This paper sheds light on the innovative formal verification approach to overcom e thorny  challenges of verifying caching and ordering mechanisms within modern SoCs, where design complexity arises from highly interconnected components, diverse data structures, intricate pipeline stages, and sophisticated ordering rules governing request handling. Traditional methodologies including dynamic simulation techniques and testing struggle to expose the full spectrum of scenarios and subtle corner cases required for robust verification. In this paper, we introduce generic and scalable techniques that tackle these challenges, resulting in a holistic end -to-end solution for the formal verification of caching and ordering logic. Our proposed framework, validated through application, uncovered over 100 bugs — including numerous critical and deeply buried corner -case issues—and enabled timely signoff. These results underscore the significance and effectiveness of formal verification in handling the challenges of complex designs and achieving comprehensive verification in advanced SoC architectures. I. |
| 4A1 DVConIndia2025 Final Paper 2911 | —The need for complex computing networks are on the ri se and this  calls for more advanced logic incorporated on a smaller form factor.  Size of monolithic SoCs for Generative AI, hyperscalers & enterprise grade data-centre applications is becoming too big for manufacturability . With increasing complexity and number of ga tes, simulation time is also increasing and the adoption of ne xt-gen verification  techniques are on the rise. With the emergence of complex multi-die SoC designs to tackle yield issues on advanced manufacturing nodes, the challenges in verification have increased manifold. This involves integration of pre-verified IPs, sub-systems, and partially verified chiplets using multiple vendor simulator platforms and Verification IPs. Artificial Intelligence (AI) is at the forefront of ASIC breakthroughs. With time to market being a critical factor, adherence to aggressive schedules has become the new normal. Rebuilding these complex verification environments onto a multi-die SoC testbench in the given timelines is extremely challenging. Universal Chiplet Interconnect Express (UCIe) is an open industry standard, multi-protocol, high-bandwidth (up to 64 GT/s per lane), die -to-die (chiplet) interconnect that standardizes inter -die communication on-package. With |
| 4A2 DVConIndia2025 Final Paper 5352 | - The BootROM, implemented as a small mask ROM or write -protected flash memory embedded within the processor chip is responsible for executing the first code when system is powered-up or reset. BootROM code is responsible for fetching all software binaries from external devices which includes the BL (Boot Loader), authenticate it and keep it in the system RAM. To enhance system security, the BootROM integrates AES (Advanced Encryption Standard) and SHA (Secure Hash Algorithm) in the chip, checks the loaded BL for security, and provides personalized key management for the chips to prevent from cyber-attack. The ultimate goal of BOOTING is to fetch all BLs from external device and to bring up the OS (Operating System).  In the ever evolving era of chip manufacturing, conventional approaches of achieving functionality, form factor, cost and much demanding power/performance goals pave the way of transitioning to smaller process nodes [1]. However, manifold increase in compute/performance requirement has pushed the monolithic System on Chip (SOC) to sizes that are challenging to fabricate with acceptable yields. Additionally, the diminishing returns of advanced nodes have made it economically impractical to accommodate all o f the required logic, IO and memory for compute intensive applications within the limits of manufacturing equipment [1]. To address these limitations, chip designers are embraced with multi-chip and multi-die designs where larger designs are partitioned into smaller designs often referred as chiplets. These chiplets are then integrated into a single package for multi-die and different packages for multi -chip to achieve the desired form factor and power goals. Additionally, these approaches provide both scalability and flexibility to cater to different market segments and specific needs. Despite clear advantages of Multi -die/Multi-chip systems, numerous novel and distinctive challenges need to be addressed. Key verification challenges include availability of a scalable test bench to realize multi-chip/multi-die simulation, infrastructure limitation, homogeneous(multi-die) and heterogeneous(multi-chip) design and increase in simulation run time with debug complexities. This paper proposes a unique scalable app roach called “DISTRIBUTED SIM” to overcome all mentioned challenges within constrained verification timeframe. DISTRIBUTED SIM provides a distributed simulation approach where each individual die/chiplet has its own simulation and all simulations communicate with each other over socket at a specified time. Preliminary result shows a 10x improvement in test bench development, 20-30 % improvement in simulation time, 15 % improvement in processor usage and 10x reduction in resource requirement over a conventional single wrapper approach of   multi-die/multi-chip simulation. All these promising figures claim DISTRIBUTED SIM as a robust approach towards multi-die/multi-chip verification. I. |
| 4A3 DVConIndia2025 Final Paper 0314 | - The increasing adoption of multi -die designs in High -Performance Computing (HPC) poses significant verification challenges. These integrated systems combine multiple dies within a single package to function as a unified System-on-Chip (SoC), offering scalability, high bandwidth, and energy efficiency. However, verifying the correctness and functionality of these complex systems is a daunting task. This paper presents a novel approach using Multi -Die Co - Simulation Framework, a parallel computing te chnique that enables efficient verification of complex multi -die systems. Our work demonstrates the effectiveness of this technology in reducing simulation time, improving verification efficiency, and enhancing scalability, making it an ideal solution for next-generation HPC systems. |
| 4B2 DVConIndia2025 Final Paper 6824 | : The escalating complexity of modern IP and SoC designs has given rise to increasingly intricate reference models. These models are critical for replicating design behaviour and guaranteeing correctness, but their development can be a time-consuming and laborious process. Fortunately, Python's simplicity and flexibility make it an ideal choice for crafting these models with greater speed and concision. In this paper, we delve into the various methods for bridging the gap between Python and System Verilog, exploring their respective advantages and disadvantages. By examining the trade-offs between these approaches, we aim to provide a comprehensive understanding of the best practices for efficient verification flows. I. |
| A Novel Configurable UVM Architecture To Unlock 1.6T Ethernet Verification Sameh Mahmoud | N/A |
| A UVM Testbench for Exploring Design Margins of AnalogMixed Signal Circuits A PCI Express Receiver Detection Circuit Example | N/A |
| AI Pair or Despair Programming Using Aider to build a VIP with UVM SV and PyUVM | N/A |
| DVConEU 2025 paper 135 | —The verification of complex designs  which include big control and protocol state -machines has always been a challenge. Although SystemVerilog and UVM have brought to the verification community the ability to create constrained random scenarios by means of test sequences and sequence libraries, the management of complex protocols from those sequences often lead s to spaghetti code, hard to maintain sequence libraries, directed tests or worse, not verifying all the targeted features. In parallel, we have seen the emergence of graph-based approaches resulting in the development of the PSS (Portable Stimulus Standard) , bringing the capacity to think differently a bout the use case scenarios, allowing them to be combined in a much bigger picture and enabling cross platform reuse. Those techniques are efficient at System Level  and for verification of complex SoC.  However, using them requires learning a new language, integrating new tools and are therefore less of an added value in complex IPs and subsystem projects already using UVM. Meanwhile, SystemVerilog provides interesting language constructs that can leverage standard UVM sequences for a more efficient constrained random generation, therefore accelerating the coverage closure of complex state machine designs. On the generation side, the randsequence keyword is powerful in generating graph-based scenarios providing that we stick to a well-structured template. Using its full capabilities  and adding a few tricks  we can even enable fully controlled graph explorations as well as automatic coverage closure completion, whether the design is fully controlled by the testbench or the testbench react s to the design behavior . On the coverage side, SystemVerilog covergroups allow the creation of state and state transition coverage. It is also possible to query the covergroups to check the completion of the scenarios. Integrated into a UVM sequence, we can define an automatic coverage closure graph-based scenario which will automatically cover all the required states and transitions of the design under test. This paper presents the overall approach, proposes an application proven template for the randsequence graph- based UVM sequence and leverages this by automating the template generation. A concrete example, based on the PCIe link training state-machine is then presented. The generation script and case example base class will be later provided as a gitlab project link  and an online generation tool available on www.aedvices.com/gbug (Graph Based UVM Generation) |
| DVConEU 2025 paper 136 | — This paper presents a UVM testbench  for characterizing the design margins of analog/mixed-signal (AMS) circuits by finding the worst-case deviation of the circuit ’s response across a continuous-valued parameter space. The testbench combines a reactive stimulus technique with a Bayesian optimization algorithm to efficiently and adaptively explore the parameter space. Using a PCI Express receiver detection circuit as a case study, of which analog components are modeled in SystemVerilog with XMODEL primitives, the paper demonstrates how the testbench can identify design points that maximize margin and assess their sensitivity to secondary operating conditions . This approach enables adaptive, coverage -driven AMS verification, supporting more automated and scalable margin analysis in complex mixed-signal systems. |
| DVConEU 2025 paper 143 | —These days the complexity of designs is growing faster than ever. This leads to the requirement of dividing and conquering the verification task in order to cope with  the complexity. Besides formal verification of the individual blocks , random verification is often still required to cover scenarios that cannot be covered formally due to complexity. For the random verification a reference model is required in order to check if the response of the design is in line with the specification.  The Universal Verification Methodology  (UVM) provides concepts to build up a testbench hierarchical to reuse verification environments in a bottom-up divide and conquer verification flow. The vertical reuse of reference models, however, is not really documented in UVM. This paper shows an approach on how the vertical reuse of reference model can be achieved. (Style: Abstract) |
| DVConEU 2025 paper 152 | —Processor verification faces significant challenges in state- space explosion and test coverage limitations, particularly in complex micro-architectures. Formal verification provides precise correctness guarantees but is constrained by computational overhead and scalability issues. Conversely, simulation-based approaches, including constrained- random verification and fuzz testing, provide scalability but often lack systematic guidance to effectively cover critical design regions and rarely exercised state transitions. To overcome these challenges, we propose Formal-Guided Test Sequence Optimization (FGTSO), a framework that integrates formal verification with simulation to systematically target coverage holes and enhance verification efficiency. FGTSO mitigates false alarms by refining formal assumptions and resolving black-box (BBOX) limitations through abstraction modeling. By continuously align- ing formal and simulation environments, FGTSO reduces test redundancy while enabling precise corner-case exploration. This approach enhances verification completeness, efficiently covering hard-to-reach design be- haviors that traditional methodologies often overlook. Experimental results on the CV A6 RISC-V core show that FGTSO achieved 99.91% branch coverage within 8 days, which is 4.41% higher than HyPFuzz’s 95.5%, effectively covering 98% of the previously uncovered regions. Furthermore, within 10 days, FGTSO achieved 100% coverage across all key metrics, including line, toggle, condition, and branch coverage. These results validate FGTSO’s ability to identify complex corner-case behaviors that traditional methods fail to reach, significantly enhancing verification completeness and efficiency. |
| Efficient Coverage Optimization with Formal Guided Testcase Generation in UVM Verification | N/A |
| P13 DVConIndia25 Final PPT 4118 | N/A |
| P14 DVConIndia2025 Final Paper 4790 | - Design verification is a highly time consuming process for design as well as verification engineers. It involves huge manual effort even with the best of the tools available in the  market. In today’s world where NLP and LLMs are widely being used to solve problems in digital design verification, we propose TiDe model which can streamline the verification process at various levels. The model takes timing diagram as input directly fro m spec and generates: 1. Unit testcases for basic testing and 2. Assertions for Formal verification. When fine -tuned properly, TiDe model is highly effective in reducing the testbench development effort for both the design and verification engineers approximately 80% and 60% respectively. |
| P14 DVConIndia25 Final PPT 4790 | N/A |
| P15 DVConIndia2025 Final Paper 4873 | - In the evolving landscape of design verification, maintaining code quality and adherence to Universal Verification Methodology (UVM) standards is paramount. AMIQ Verissimo, a powerful linting tool, plays a crucial role in identifying structural issues like coding errors, style violations and non-standard constructs in System Verilog and UVM testbenches. This paper presents an automation framework tailored to AMIQ Verissimo, designed to streamline the linting process, minimize manual effort, and standardize the execution flow. By integrating AMIQ Verissimo into an automated environment the framework enhances verification efficiency, enforces coding consistency, and significantly reduces turnaround time. The proposed solution empowers verification teams with a faster, more reliable, and scalable linting methodology. I. |
| P15 DVConIndia25 Final PPT 4873 | N/A |
| P17 DVConIndia2025 Final Paper 5053 | - Modern-day IP-development must deal with ever changing specification documents. Specifications are not fully defined before the IP development phase; they tend to evolve along with the development. IP also goes through multiple variations accordingly before landing on the final version. This causes a substantial number of changes to the Verification IP, which accommodates multiple protocols and products. Catering to these variations without breaking existing features clutters the testbench with several conditionals, making the code fragile and maintenance a time - consuming task. The objective is to ensure that new code addition never breaks the functionality of old and tested code. Chain-of-Responsibility (CoR ) is an Object-Oriented Design Pattern , which can help achieve this objective. This paper discusses how Chain-of-Responsibility was used in IP that supported multiple protocols and products and its relative merits over traditional factory methods. |
| P17 DVConIndia25 Final PPT 5053 | N/A |
| P19 DVConIndia2025 Final Paper 6547 | - Formal Verification provides powerful ways of finding corner case scenarios in many design types. This is due to the inherent nature of formal verification where the randomness of the inputs to the DUT help uncover scenarios where the design can fail based on the checks written. While this strength is best utilized by lightly constraining the inputs to see what the formal tools can uncover, they can pose a challenge when the design under test expects the inputs to be ordered across interfaces in a strict order. In this paper we talk about how we tackled this challenge using a novel method of using symbolic variables in a multi -dimensional map structure. |
| P19 DVConIndia25 Final PPT 6547 | N/A |
| P20 DVConIndia2025 Final Paper 6976 | -Functional Safety is a challenging and an absolute requirement of the automotive and industrial SoCs and ASICs that need to show compliance to ISO26262 and IEC61508 respectively. It adds a new dimension to the world of pre-silicon design verification (DV) and hence requires purpose-built tools, flows and methods (TFM) to achieve the same. Conventional DV TFMs are nearly sufficient to show compliance with the systematic failure requirements, but the random hardware failure requirements demand newer EDA solutions. Through the learnings from real-life use cases of multiple ASIL-D / SIL3 SoCs and ASICs development, an ideal FuSa verification solution is proposed that leverages innovative FuSa aware debug and AI/ML technologies. I. |
| P20 DVConIndia25 Final PPT 6976 | N/A |
| Unified UVM Testbench Integrating Random Directed and Pseudo Random Verification Capabilities | N/A |
