# DVCon PDF Abstracts

This file is generated from DVCon PDFs using a simple OCR-capable 
pipeline. For best results, install the optional dependencies:

```bash
pip install pypdf pdf2image pytesseract
```

You may also need to install system packages for Poppler and 
Tesseract OCR (see the respective project documentation).

The canonical project abstract and search-friendly description are 
maintained in the main `README.md`. This file is intended as a 
supporting appendix that lists per-paper abstracts extracted from 
DVCon PDFs.

| Paper | Abstract |
| --- | --- |
| UVM Working Group Release 1800.2 2020 2.0 Library | N/A |
| Unleashing the Potential of Agentic AI Within Design Functional Verification | N/A |
| Unified UVM Testbench Integrating Random Directed and Pseudo Random Verification Capabilities | N/A |
| Unified Architecture of L1 L2 Cache with Low Power Extensions for Multi Core UVM based Library Package | N/A |
| Tutorial 5B Introduction of IEEE 1801 2024 UPF4 0 | N/A |
| TS2A Integrating L1 L2 Cache for multi Core UVM based extended Low Power Library Package slide | N/A |
| TS2A Integrating L1 L2 Cache for multi Core UVM based extended Low Power Library Package paper | — This Paper demonstrates  the continuum for multi -Core architecture integrating UPF based Low Power methodologies and strategies to L1 & L2 Cache in Off, Sleep, Dormant and Retention modes  within the UVM Low Power Package by addressing limitation of previous works (referenced) to incorporate multi -Core low power libraries (which has the classes for SOC environm ent Devices, Buses and Memory) for low power strategies which may be deployed in UVM Agents executing within Run Phase with in -built ASM routines to sequence PowerUp/Down for multi -Core L1 & L2 Cache and incorporate these within low power UVM classes using SystemVerilog and DPI. |
| The Test Bench Factory Building Verification Environments Faster Better Smarter | N/A |
| T 2B Liberating Functional Verification | N/A |
| SW 4B 2 PSS In Action | N/A |
| SW 4A 1 CDC RDC Format Update | Model Farhad AHMEDJebin MOHANDAS Bill GASCOYNE Kranthi PAMARTHI Chetan CHOPPALI SUDARSHAN Nomita GOSWAMI Aiyush AGGARWAL Rahul PARASHAR Ashish SONI Jean-Christophe BRIGNONE Joachim VOGES John JEBAKUMAR Jan HAYEK Suman CHALANA 1 Devender KHARI Don MILLS Presenters |
| SW 3B 2 Agentic AI Functional Verification | N/A |
| SW 3B 1 Formal Verification AI | N/A |
| SW 3A 2 Addressing Protocol Verification AI HPC | N/A |
| SW 2A 2 Veloce proFPGA | N/A |
| slides 184 Trustworthiness Evaluation of Deep Learning Accelerators Using UVM based Verification with Error Injection | N/A |
| Scalable agile processor verification using SystemC UVM and friends | N/A |
| Reduce Reuse Reverify An efficient approach to transition formal verification environments from PCIe Gen6 to Gen7 | N/A |
| Real Number Voltage aware behavioral modeling and verification of SRAM subsystem with Unified Power Format UPF | N/A |
| Pre Silicon Verification of Software Safety Mechanisms A Hybrid Approach with SPI and NVDLA Case Studies | N/A |
| PP2.3 Slide Maximizing Verification Productivity Using UVM and Dynamic Test Loading | N/A |
| PP2.2 Slide Having Your Cake and Eating It Too Programming UVM Sequences with DPI C | N/A |
| PP2.2 Paper Having Your Cake and Eating It Too Programming UVM Sequences with DPI C | —Blending SystemVerilog UVM and SystemVerilog DPI-C is a powerful way to create or reuse pre-existing verification environments. This paper describes the mechanisms and methods and syntax needed, including writing tasks and functions in both the SystemVerilog interface and the UVM sequences. |
| Paper 1.3 Scoreboards and Checkers Memory TLB and Cache | N/A |
| Paper 1.2 UVM based extended Low Power Library package with Low Power Multi Core Architectures paper | — This paper explores integration of Unified Power Format (UPF) and Universal Verification Methodology (UVM) to simplify the functional verification and power management process. Specifically, focuses on incorporating low power design features within multi-Core architectures using in-built low power routines in Assembly Language (ASM). The proposed Low Power UVM Package contains classes for SOC environment like Devices, Buses, and Memory, which can implement low power strategies using SystemVerilog and DPI extension within proposed Low Power UVM Package during the Run Phase of UVM Agents. The aim is to enable efficient power management and bridge the gap between functional verification and power management engineers. |
| P20 DVConIndia25 Final PPT 6976 | N/A |
| P20 DVConIndia2025 Final Paper 6976 | -Functional Safety is a challenging and an absolute requirement of the automotive and industrial SoCs and ASICs that need to show compliance to ISO26262 and IEC61508 respectively. It adds a new dimension to the world of pre-silicon design verification (DV) and hence requires purpose-built tools, flows and methods (TFM) to achieve the same. Conventional DV TFMs are nearly sufficient to show compliance with the systematic failure requirements, but the random hardware failure requirements demand newer EDA solutions. Through the learnings from real-life use cases of multiple ASIL-D / SIL3 SoCs and ASICs development, an ideal FuSa verification solution is proposed that leverages innovative FuSa aware debug and AI/ML technologies. I. |
| P2 DVConIndia25 Final PPT 1418 | N/A |
| P2 DVConIndia2025 Final Paper 1418 | -Formal verification's adoption is hindered by complexity and scalability issues, limiting its use to small design blocks and creating a gap for complex systems. This paper proposes an AI -enabled formal verification approach, integrating Artificial Intellig ence (AI) into every step, from property extraction to proof convergence, leveraging Large Language Models (LLMs) for automated property generation, abstract model creation, and building a comprehensive database of verification information. AI intelligently classifies, sorts, and reuses properties to reduce complexity, enhancing formal property verification (FPV) and datapath validation (DPV) through design decomposition and formal engine orchestration. Convergence techniques such as abstraction and propert y decomposition are employed to address state - space explosion, ensuring faster and more accurate verification. Comprehensive verification is ensured through analysis of coverage metrics, including cone -of-influence, formal core, and testbench analysis, gua ranteeing thorough verification before sign-off. AI-driven analytics and detailed reporting simplify debugging and design optimization, while the systematic flow improves accuracy, scalability, and productivity, ultimately achieving faster, confident signoffs. |
| P19 DVConIndia25 Final PPT 6547 | N/A |
| P19 DVConIndia2025 Final Paper 6547 | - Formal Verification provides powerful ways of finding corner case scenarios in many design types. This is due to the inherent nature of formal verification where the randomness of the inputs to the DUT help uncover scenarios where the design can fail based on the checks written. While this strength is best utilized by lightly constraining the inputs to see what the formal tools can uncover, they can pose a challenge when the design under test expects the inputs to be ordered across interfaces in a strict order. In this paper we talk about how we tackled this challenge using a novel method of using symbolic variables in a multi -dimensional map structure. |
| P17 DVConIndia25 Final PPT 5053 | N/A |
| P17 DVConIndia2025 Final Paper 5053 | - Modern-day IP-development must deal with ever changing specification documents. Specifications are not fully defined before the IP development phase; they tend to evolve along with the development. IP also goes through multiple variations accordingly before landing on the final version. This causes a substantial number of changes to the Verification IP, which accommodates multiple protocols and products. Catering to these variations without breaking existing features clutters the testbench with several conditionals, making the code fragile and maintenance a time - consuming task. The objective is to ensure that new code addition never breaks the functionality of old and tested code. Chain-of-Responsibility (CoR ) is an Object-Oriented Design Pattern , which can help achieve this objective. This paper discusses how Chain-of-Responsibility was used in IP that supported multiple protocols and products and its relative merits over traditional factory methods. |
| P15 DVConIndia25 Final PPT 4873 | N/A |
| P15 DVConIndia2025 Final Paper 4873 | - In the evolving landscape of design verification, maintaining code quality and adherence to Universal Verification Methodology (UVM) standards is paramount. AMIQ Verissimo, a powerful linting tool, plays a crucial role in identifying structural issues like coding errors, style violations and non-standard constructs in System Verilog and UVM testbenches. This paper presents an automation framework tailored to AMIQ Verissimo, designed to streamline the linting process, minimize manual effort, and standardize the execution flow. By integrating AMIQ Verissimo into an automated environment the framework enhances verification efficiency, enforces coding consistency, and significantly reduces turnaround time. The proposed solution empowers verification teams with a faster, more reliable, and scalable linting methodology. I. |
| P14 DVConIndia25 Final PPT 4790 | N/A |
| P14 DVConIndia2025 Final Paper 4790 | - Design verification is a highly time consuming process for design as well as verification engineers. It involves huge manual effort even with the best of the tools available in the  market. In today’s world where NLP and LLMs are widely being used to solve problems in digital design verification, we propose TiDe model which can streamline the verification process at various levels. The model takes timing diagram as input directly fro m spec and generates: 1. Unit testcases for basic testing and 2. Assertions for Formal verification. When fine -tuned properly, TiDe model is highly effective in reducing the testbench development effort for both the design and verification engineers approximately 80% and 60% respectively. |
| P13 DVConIndia25 Final PPT 4118 | N/A |
| P11 DVConIndia25 Final PPT 3773 | N/A |
| P11 DVConIndia2025 Final Paper 3773 | - Verification of RISC-V processors demands stress-testing well beyond ISA compliance. This paper presents a novel methodology leveraging the Portable Stimulus Standard (PSS) to generate sophisticated stress test scenarios targeting critical CPU functionalities including data hazards, branch prediction mechanisms, and resource contention. Our approach creates adaptable and reusable test scenarios that can be applied across various CPU architectures, from simple in-order designs to complex out-of-order superscalar implementations. We demonstrate a proof -of-concept implementation on a single -cycle in -order RISC -V core to validate the methodology's effectiveness while maintaining clarity in results interpretation. Early evaluation shows significant improvements in functional coverage and the ability to uncover subtle microarchitectural issues that traditional directed testing approaches often miss. This work establishes a foundation for scalable processor verification practices in the rapidly expanding RISC-V ecosystem. I. |
| P10 DVConIndia25 Final PPT 3645 | N/A |
| P10 DVConIndia2025 Final Paper 3645 | -Modern automotive displays use DisplayPort (DP) or Embedded DisplayPort (eDP) to carry video data from the central CPU to the displays. VESA's DisplayPort Automotive Extensions (DP AE) protocol adds automotive-grade functional safety and security to the existing DP and eDP standards. Chip manufactures have already started adopting DP AE for designing chipsets that will be part of future vehicles. Thus, there arises a need for an optimized testbench to ensure product quality and shorter time to market. This paper discusses the art of constructing a scalable testbench to mitigate the verification challenges in DP AE by leveraging existing DP testbench. |
| Out of The Box Techniques for Data Path Verification | N/A |
| Minimally Intrusive Safety and Security Verification of Rust RTIC Applications | N/A |
| Migrating from UVM to UVM MS | N/A |
| Introduction to the Apheleia Verification Library | N/A |
| Introduction of IEEE 1801 2024 UPF4.0 improvements for the specification and verification of low power | N/A |
| Implementing Functional Coverage for Analog IPs in Mixed Signal Verification Environments | N/A |
| Fully Automated Verification Framework for Configurable IPs From Requirements to Results | N/A |
| FPGA Firmware Verification a common approach for simulation and hardware tests | N/A |
| Exploring the Limits of Vertical Reuse Automation in PSS Driven SoC Verification | N/A |
| Exploring New Frontiers of High Performance Verification with UVM AMS | N/A |
| Expediting Coverage Closure in Digital Verification with the Portable Stimulus Standard PSS | N/A |
| Efficient Coverage Optimization with Formal Guided Testcase Generation in UVM Verification | N/A |
| DVConIndia2025 Tutorials Final ppt 0156 | N/A |
| DVConEU 2025 paper 95 | N/A |
| DVConEU 2025 paper 172 | —In the context of formal methods, symbolic execution stands out as highly automatic, allowing static code analysis to be adopted by users without domain speciﬁc expertise or training. Symex is a symbolic execution framework speciﬁcally targeting anal- ysis of embedded software with requirements to safety, security, and dependability. However, so far, the execution engine of Symex has been executing the LLVM Intermediate Representation(LLVM-IR). Because no consistent mapping between LLVM-IR and actual machine code exists, Symex has been unable to provide guarantees to runtime properties of the system, such as Worst-Case Execution Time (WCET). In this paper, we extend Symex by moving the execution engine to General Assembly (GA), an Intermediate Representation (IR) capable of capturing the semantics of Instruction Set Architectures (ISAs), along with their non-functional properties, e.g. per-instruction execution time, or power consumption. Symex lifts the ELF binary to GA and explores all reachable paths without approximations, thus it is able to provide guarantees to runtime characteristics of the system, taking into account architecture speciﬁc behaviour and compiler backend/linker optimizations. Furthermore, we introduce the EASY system analysis framework, which uses Symex as a symbolic execution backend, and therefore grants the same veriﬁcation capabilities. EASY can provide analysis for response time, task memory isolation and application stack memory utilization. We demonstrate the feasibility of GA-based symbolic execution by modelling the full ARMv6 and most of the v7 ISA, as well as the RV32I ISA. Leveraging on the Rust RTIC framework, we demonstrate that EASY is capable of automatically determining the schedulability, worst- case stack memory utilization and task memory isolation properties of the system. |
| DVConEU 2025 paper 169 | — As the complexity of semiconductor designs continues to grow, the verification effort has expanded significantly in both time and resources. Traditional automation techniques —such as constrained random testing, assertion-based verification, and m etric-driven verification —have helped streamline the process but remain heavily manual in nature when it comes to planning, debug, and stimulus generation. The emergence of Generative AI (Gen -AI) has shown promise in domains such as software engineering and natural language processing; however, its application in hardware verification remains largely exploratory. Most attempts are ad hoc, lacking a structured way to assess where and how Gen -AI can be embedded into the verification lifecycle. This paper buil ds on these explorations and proposes a systematic framework that identifies, categorizes, and evaluates Gen-AI opportunities across the verification flow. |
| DVConEU 2025 paper 164 | —As  System-on-Chip  (SoC)  designs  integrate  increasingly  diverse  and  software-driven  components, verification reuse becomes critical for managing complexity across block, subsystem, and system levels. Despite advances in automation, the theoretical and practical boundaries of vertical reuse in complex SoC integrations remain underexplored. This paper investigates the limits of vertical reuse in Portable Stimulus Standard (PSS) workflows by applying a static analysis–based toolchain to a real-world SoC design. In PSS, reusable verification intent is captured as Portable Models (PMs), which combine abstract scenario definitions with realization-layer bindings to design interfaces. While prior work demonstrated that static analysis enables vertical reuse on designs of various sizes and complexities, this study examines how far such reuse can be extended as integration progresses toward full SoC levels. Using a Keccak-based cryptographic accelerator integrated through two distinct architectures — loosely coupled and tightly coupled — the feasibility of vertical reuse across six integration contexts is evaluated. The results show that static connectivity analysis supports reuse across hierarchy levels but that SoC-level reuse increasingly intersects with software-driven control, requiring additional modeling effort. This case study highlights both the reach and the limits of structural automation in enabling PM reuse, providing insight into when automation suffices and where additional modeling effort is required. |
| DVConEU 2025 paper 163 | — Verifying software safety mechanisms for peripheral IPs presents a significant challenge during pre- silicon development, as these mechanisms require the entire System-on-Chip to execute. A software safety mechanism is a piece of software that detects hardware faults. This paper presents a novel hybrid verification approach that addresses this challenge by combining hardware emulation with software virtualization to enable comprehensive fault injection campaigns before silicon availability. Our architecture interfaces an RTL emulator running peripheral IP blocks with a host machine executing a virtualized CPU environment through a transaction-level interface. This hybrid approach provides signal-level accuracy while delivering practical execution times for fault campaigns. We demonstrate its effectiveness through two case studies: a memory-mapped SPI controller and the more complex NVIDIA Deep Learning Accelerator (NVDLA). For NVDLA, we implemented a progressive multiple-point fault injection methodology, revealing that neural accelerators demonstrate moderate vulnerability to single-point fault injection (10% accuracy loss), and become increasingly susceptible with multiple simultaneous faults injected (95% accuracy loss with 32 faults). Experimental results show the platform achieves 10× speedup over fully-emulated environments and 1000× acceleration compared to RTL simulation approaches, while utilizing only 0.12% of available resources. For the SPI controller, combining safety mechanisms achieved up to 96.3% fault detection rate, which aligns with ISO 26262 estimation for safety mechanisms. The methodology enables early vulnerability detection and refinement of safety mechanisms, supporting the timely development of ASIL-compliant automotive systems. |
| DVConEU 2025 paper 158 | —The increasing complexity of modern digital circuits requires robust verification to ensure reliability and prevent costly failures. Among various formal verification methods, Symbolic Computer Algebra (SCA) offers a powerful approach by representing circuits using polynomials. However, a significant challenge in SCA verification is the exponential term expansion during substitution, which drastically increases verification time. This paper addresses this challenge by investigating the impact of circuit transformations on SCA verification efficiency. We propose a transformation-aided verification process, showcasing its effectiveness through a case study on Multiply-and-Accumulate MAC-based designs. Specifically, we examine the transformation of NAND/NOR-based designs and demonstrate its substantial impact on verification time for certain MAC circuits. Experi- mental results reveal interesting findings, notably a many-fold performance gain for some benchmarks. |
| DVConEU 2025 paper 152 | —Processor verification faces significant challenges in state- space explosion and test coverage limitations, particularly in complex micro-architectures. Formal verification provides precise correctness guarantees but is constrained by computational overhead and scalability issues. Conversely, simulation-based approaches, including constrained- random verification and fuzz testing, provide scalability but often lack systematic guidance to effectively cover critical design regions and rarely exercised state transitions. To overcome these challenges, we propose Formal-Guided Test Sequence Optimization (FGTSO), a framework that integrates formal verification with simulation to systematically target coverage holes and enhance verification efficiency. FGTSO mitigates false alarms by refining formal assumptions and resolving black-box (BBOX) limitations through abstraction modeling. By continuously align- ing formal and simulation environments, FGTSO reduces test redundancy while enabling precise corner-case exploration. This approach enhances verification completeness, efficiently covering hard-to-reach design be- haviors that traditional methodologies often overlook. Experimental results on the CV A6 RISC-V core show that FGTSO achieved 99.91% branch coverage within 8 days, which is 4.41% higher than HyPFuzz’s 95.5%, effectively covering 98% of the previously uncovered regions. Furthermore, within 10 days, FGTSO achieved 100% coverage across all key metrics, including line, toggle, condition, and branch coverage. These results validate FGTSO’s ability to identify complex corner-case behaviors that traditional methods fail to reach, significantly enhancing verification completeness and efficiency. |
| DVConEU 2025 paper 151 | — Complex automotive ASICs and SoCs require rigorous verification and testing, which can be both time - consuming and costly. To accelerate this process, modern ASIC and SoC development flows utilize parallel design representations at different levels of abstraction, commonly virtual prototypes (VPs) alongside RTL and mixed-signal design. However, failing to maintain consistency among design representations can undermine the advantages of utilizing them in parallel, compromising quality and delaying time -to-market. This paper addresses the critical issue of ensuring consistency among system specifications, mixed-signal design representation, VPs, and software throughout ASIC and SoC development. Our approach focuses on applying the same use case scenarios across all design representations, which in turn enhances the verification of digital and mixed -signal designs using the real-world scenarios themselves and allows for the validation of overall system functionality when hardware and software designs operate together. We propose an automated method for use case mapping between abstraction levels (VP to RTL and vice versa) relying on  signal trac es recorded in VCD file besides small set of refinement information  to facilitate essential timing adjustments and manage signal interdependencies. To demonstrate the effectiveness of our approach, we conducted a case study to verify a production automotive ASIC design, reporting quantitative measures of accuracy and effort reduction, as well as qualitative findings that enhanced verification strategy at early stage of the project. |
| DVConEU 2025 paper 144 | —Artificial Intelligence (AI) and Machine Learning (ML) have been increasingly adopted to enhance engineering productivity across various domains. In the context of hardware design and verification, where workflows are often complex, repetitive, and resource-intensive, Generative AI (GenAI) has emerged as a promising approach to support various engineering tasks. This paper investigates the use of GenAI in areas such as requirements engineering, verification setup, debugging, coverage closure, and related activities. Through prompt-driven and context-aware interactions, GenAI can help streamline the verification workflow, reduce manual effort, and allow engineers to focus on more complex and creative problem-solving. Preliminary evaluations demonstrate productivity improvements of up to 30% when GenAI is integrated into the verification process. |
| DVConEU 2025 paper 143 | —These days the complexity of designs is growing faster than ever. This leads to the requirement of dividing and conquering the verification task in order to cope with  the complexity. Besides formal verification of the individual blocks , random verification is often still required to cover scenarios that cannot be covered formally due to complexity. For the random verification a reference model is required in order to check if the response of the design is in line with the specification.  The Universal Verification Methodology  (UVM) provides concepts to build up a testbench hierarchical to reuse verification environments in a bottom-up divide and conquer verification flow. The vertical reuse of reference models, however, is not really documented in UVM. This paper shows an approach on how the vertical reuse of reference model can be achieved. (Style: Abstract) |
| DVConEU 2025 paper 139 | — Algorithmic datapath designs are typically modelled at a high level in C/C++ and can be verified at early stages before the corresponding RTL design is ready. The criticality for the equivalence verification of datapath designs (RTL) with their  reference high -level C/C++ models, is well accepted.  Simulation-based approaches like DPI or scoreboarding suffer from challenges of achieving verification completeness. The means of verifying this equivalence is shifting from simulation to formal gradually, thanks to new solver capabilities available in commercial formal tools and the exhaustive nature of formal.  However complex image processing algorithms often run into  the challenge of compiling i.e. building  the formal model from C++  of in reasonable time. Also, we often struggle to achieve proof convergence on the equivalence check targets, even if we manage to compile the C/C++ models. While formal methods can be potentially effective, the right methodology is required to  overcome these challenges and scale to larger and complex designs. In this paper, we propose several generic techniques  that have helped us to generate formal model s for FFT and  Decompression units in a couple of minutes and converge on the  properties which was impossible to achieve out of the box. In  this process, we uncovered bugs that  traditional verification methods failed to detect . All these techniques are reusable. This paper presents details of these techniques by taking an FFT and a Decompression algorithm as an example. |
| DVConEU 2025 paper 136 | — This paper presents a UVM testbench  for characterizing the design margins of analog/mixed-signal (AMS) circuits by finding the worst-case deviation of the circuit ’s response across a continuous-valued parameter space. The testbench combines a reactive stimulus technique with a Bayesian optimization algorithm to efficiently and adaptively explore the parameter space. Using a PCI Express receiver detection circuit as a case study, of which analog components are modeled in SystemVerilog with XMODEL primitives, the paper demonstrates how the testbench can identify design points that maximize margin and assess their sensitivity to secondary operating conditions . This approach enables adaptive, coverage -driven AMS verification, supporting more automated and scalable margin analysis in complex mixed-signal systems. |
| DVConEU 2025 paper 135 | —The verification of complex designs  which include big control and protocol state -machines has always been a challenge. Although SystemVerilog and UVM have brought to the verification community the ability to create constrained random scenarios by means of test sequences and sequence libraries, the management of complex protocols from those sequences often lead s to spaghetti code, hard to maintain sequence libraries, directed tests or worse, not verifying all the targeted features. In parallel, we have seen the emergence of graph-based approaches resulting in the development of the PSS (Portable Stimulus Standard) , bringing the capacity to think differently a bout the use case scenarios, allowing them to be combined in a much bigger picture and enabling cross platform reuse. Those techniques are efficient at System Level  and for verification of complex SoC.  However, using them requires learning a new language, integrating new tools and are therefore less of an added value in complex IPs and subsystem projects already using UVM. Meanwhile, SystemVerilog provides interesting language constructs that can leverage standard UVM sequences for a more efficient constrained random generation, therefore accelerating the coverage closure of complex state machine designs. On the generation side, the randsequence keyword is powerful in generating graph-based scenarios providing that we stick to a well-structured template. Using its full capabilities  and adding a few tricks  we can even enable fully controlled graph explorations as well as automatic coverage closure completion, whether the design is fully controlled by the testbench or the testbench react s to the design behavior . On the coverage side, SystemVerilog covergroups allow the creation of state and state transition coverage. It is also possible to query the covergroups to check the completion of the scenarios. Integrated into a UVM sequence, we can define an automatic coverage closure graph-based scenario which will automatically cover all the required states and transitions of the design under test. This paper presents the overall approach, proposes an application proven template for the randsequence graph- based UVM sequence and leverages this by automating the template generation. A concrete example, based on the PCIe link training state-machine is then presented. The generation script and case example base class will be later provided as a gitlab project link  and an online generation tool available on www.aedvices.com/gbug (Graph Based UVM Generation) |
| DVConEU 2025 paper 132 | - Most testbench environments use separate setups for random , pseudo-random, and directed verification strategies, leading to duplicate efforts and limited reusability. This fragmentation results in redundant development, inconsistent methodologies, and delays in verification cyc les. As projects progress —from directed tests early on to random exploration in the middle and pseudo -random patterns for targeted coverage closure toward the end — maintaining isolated environments becomes inefficient. The proposed solution is  a unified UVM -based testbench that integrates all verification modes into a single configurable environment. By supporting  mode selection through configuration, dynamic layering of sequences, and utilizing a reusable testbench library , this approach reduces overhead, enhances reusability from IP to SoC levels, and streamline s test development throughout the verification lifecycle. |
| DVConEU 2025 paper 131 | — Power management in modern  SoCs is becoming increasingly complex due to several evolving factor s and  design intricacies as compared to earlier times . To support this increased functionality while respecting strict power budgets  of these SoCs  , chips need to be designed to operate efficiently in various low -power modes . Such complex designs also need stringent low power verification techniques which can be performed  early in the design cycle to avoid late cycle debugging issues.  UPF IEEE 1801 standard [1] -based Soc verification for complex SoCs and subsystems plays a key role to verify the power management functions at RTL stage. This paper presents a solution of mimicking the power functional behavior using the real voltage values in digital simulations leading to voltage awareness in modeling of the power function of Ips and subsystems critical to achieve the power intent of the SoC. Using voltage-aware modeling with real-number voltage variables  allows designers to transact actual voltage values during RTL simulation thereby improving accuracy and verification coverage [2]of power critical blocks and systems. This modeling methodology is scalable and can be applied across a wide range of hard macro IPs and subsystems. This methodology is demonstrated through a memory subsystem comprising a Retention SRAM ( RETmem) and a Bias Generator (BIASG) block which is responsible for generating specific bias voltage for different RETmem operating modes as per the system’s low-power design requirements. |
| DVConEU 2025 paper 130 | —Verifying complex SoC hardware and software systems poses significant challenges, particularly when trying to balance fast verification cycles with growing design complexity. This paper explores how reinforcement learning (RL) can be applied to improve coverage closure time. We present an experiment involving the verification of a custom finite state machine (FSM), comparing results obtained through traditional constrained random verification with those enhanced by reinforcement learning. The findings demonstrate that integrating RL into the verification flow leads to a noticeable reduction in coverage closure time, showcasing the potential of machine learning to optimize and accelerate the verification process. |
| DVConEU 2025 paper 127 | — Verification signoff of Ethernet Design Intellectual Property  (IP) with vast configuration related to Speed, type of Forward Error Correction ( FEC) encoding, num ber of Serdes lanes has considerable challenges. Standalone Universal Verification Methodology (UVM) based verification is not sufficient to ensure comprehensive design validation. This paper presents a comprehensive verification strategy  using a combination of  UVM, Python- based modeling, and formal verification-based techniques. Leveraging advanced Electronic Design Automation (EDA) verification platforms including AI enhanced debug and formal tools, this work demonstrates how simulation, property checking, and custom reference modeling can be applied to validate complex IPs . The paper highlights how Machine learning (ML) based EDA tools accelerate root -cause analysis and coverage closure, while version -aware workflows ensure maintainable, reproducible regressions across design iterations. Through practical examples, this paper will show ho w each methodology complements the others, resulting in a scalable, coverage -driven, and bug -resilient verification flow for next generation Ethernet IPs. |
| DVConEU 2025 paper 124 | —As PCIe (Peripheral Component Interconnect Express) technology advances, maintaining backward compatibility while optimizing performance for next-generation interfaces has posed a significant challenge. This paper explores the reuse of two existing PCIe Gen6 formal verification environment s for PCIe Gen7 by strategically using gearbox technique. The gearbox is a critical component in managing protocol timing to accommodate Gen7’s increased bandwidth. Through systematic refinement of the formal verification setup —including protocol constraints, timing assumptions, and data path configurations —we demonstrate an efficient transition from Gen6 to Gen7 without the need for a complete environment overhaul. This approach significantly reduces verification effort and acc elerates the validation process for next-generation PCIe designs. The proven results confirm that with targeted modifications, the Gen6 verification infrastructure can effectively validate Gen7 implementations, ensuring compliance with new specifications while leveraging prior investments in formal verification. |
| DVConEU 2025 paper 118 | —The increasing competition in the semiconductor industry has created significant pressure to reduce chip prices while maintaining quality and reliability. Functional verification, particularly for configurable IPs, is a major contributor to development cos ts due to its complexity and resource -intensive nature. To address this, we propose a fully automated framework for requirements-driven functional verification. The framework automates key processes, including vPlan generation, testbench creation, regressi on execution, and reporting  in a requirement s management tool (RMT), drastically reducing verification effort. This approach accelerates development cycles, minimizes human error, and enhances coverage, offering a scalable and efficient solution to the cha llenges of verifying configurable IPs in today’s competitive market. |
| DVConEU 2025 paper 116 | — As SoC and IP designs grow increasingly complex, the demand for flexible and maintainable verification methodologies continues to rise. While SystemVerilog UVM remains the industry-standard verification framework of- fering constrained-random testing, functional coverage, and scalable environments, its steep learning curve and limited interoperability with modern software tools present notable challenges. This study explores an alternative approach using Python-based verification integrated with commercial simulators and compares it against a traditional UVM methodology. We implement two functionally equivalent verification environments for a UART IP core with an APB interface: a conventional SystemVerilog UVM testbench, and a Python-based solution combining pyuvm’s structured verification components with cocotb’s co-simulation capabilities. Both environments utilize industry-standard licensed simulators, ensuring a fair and practical comparison. Our evaluation focuses on four key metrics: simulation performance, func- tional coverage efficiency, testbench maintainability, and component reusability across projects. Generally, results shows that Python-based verification can effectively complement traditional UVM flows, especially in unit-level verification scenarios, where rapid development, dynamic stimulus generation, and seamless integration with modern software stacks offer significant advantages. Although UVM remains favorable for large-scale SoC verification due to performance and maturity, the Python-based approach presents a compelling alternative for targeted use cases in commercial verification environments. |
| DVConEU 2025 paper 115 | — Accurate data flow is the backbone of any reliable hardware design. Ensuring data integrity by preventing corruption, duplication, dropping, or reordering is key to validate system correctness. However, verifying data integrity using Formal Verification (FV) becomes highly challenging in packet-based designs where complex packing rules, shifting, and alignment introduce possibilities of subtle and hard-to-detect issues. The challenge intensifies in PCIe 6.0, where TLP Bytes are packed into Flits under strict packet packing rules involving frequent data shifting and alignment. Traditional FV data integrity techniques ineffective when designs modify or partially shift input data before sending it to output. In this paper, we present our array based novel data integrity approach called Array Centric Tracking (ACT), a scalable technique that tracks input TLP Bytes across Flits to validate data integrity and packing rules. We demonstrate various applications, benefits of ACT and caught 38 bugs using our approach including many subtle corner cases that traditional verification methods failed to catch. |
| DVConEU 2025 paper 107 | —As open-source verification gains momentum, Verilator has become a key tool for SystemVerilog -based simulation. However, the lack of full support for constrained randomization, a cornerstone of UVM-based verification, has remained a major limitation. This paper presents our extension to Verilator’s randomization capabilities, adding support for aggregated data types such as structs, unions, and arrays, with both basic and constrained randomization. We introduce an optim ized, template -based architecture that handles these complex types efficiently without compromising performance. To validate the implementation, we developed three UVM testbenches with varying structures and complexities, all verified on QuestaSim and then  tested on Verilator. The results highlight both the current capabilities and remaining limitations of Verilator in supporting UVM environments. This work closes a critical gap, advancing Verilator toward a fully capable open-source solution for UVM-based verification. |
| DVConEU 2025 paper 105 | — Code generation is a common solution for improving the productivity and adaptability of software. In the functional verification domain, for example, generators that model the hardware/software interface, or "configuration registers," of a design under verification are widely used in the industry. These generators help improve productivity, reduce repetitive tasks, ensure specification compliance, or simply overcome the "blank page syndrome”. In this paper, we present a test bench code generation approach that combines several technologies. This approach enables the creation of a fully functional SystemVerilog test bench using the Universal Verification Methodology (UVM). The generated test bench covers the three main verification aspects: stimulus generation, checking and functional coverage. This approach significantly accelerates the development process, reduces test bench errors, and facilitates the reuse of verification components. By providing a standard architecture, users can avoid a steep learning curve when starting a new project. |
| DVConEU 2025 paper 104 | — Gyroscope system is inherently complex, consisting of both mechanical and electronic parts, often with feedback loops that interact through state machine and digital controls. While tools like MATLAB are widely used for verifying system architecture, Analog Mixed-Signal (AMS) environments are more commonly employed for verifying the actual implementation and ensuring functionality. However, Digital Mixed-Signal (DMS) environment has not been extensively explored for verifying gyroscope systems, despite their potential for improving verification efficiency. This paper aims to explore the challenges of modeling a gyroscope system (ASIC and Micro-Electronical-Mechanical Systems (MEMS) ) in a DMS environment, focusing on the integration of mechanical and electronic subsystems. Furthermore, we demonstrate how DMS enhances verification efficiency by offering an integrated platform that enable better coverage. |
| DVConEU 2025 paper 102 | — This paper proposes an approach for implementing functional coverage within a UVM -based top-level testbench for analog intellectual property (IP) in ASICs. The methodology is compatible with both Digital Mixed Signal (DMS) and Analog Mixed Signal (AMS) simulations. Functional covergroups are defined in the testbench environment based on the IP specification. These covergroups help extract coverage metrics to ensure that critical chip-level features are exercised and captured by the verification pl an. A key focus of this work is the adaptation of metric -driven verification techniques —common in digital verification —for use in analog IPs. The approach supports both real number models and analog schematics in mixed -signal simulations, helping to bridge the  gap between analog and digital design and verification processes. |
| DVConEU 2025 paper 101 | —Verification of high -speed Ethernet controllers poses significant challenges due to the increasing complexity of designs that must support a wide range of configurations and data rates, from 10Mb to 1.6T. This paper presents a novel, fully automated, and configurable UVM-based verification environment tailored for a generic Ethernet controller RTL. The RTL design supports various features sets and configurations, including optional Ultra Ethernet Consortium (UEC) layers, variable SerDes widths, and diverse data rates, all driven by RTL defines generated via a builder tool. To address  testbench customization overhead and avoid manual rework for each RTL variant , a Python-based flow was developed to parse RTL defines and dynamically generate a configuratio n-specific UVM top- level using Jinja2 templates. This produces a testbench-matching Device Under Test (DUT), supporting full-controller and PCS-only modes, with parameterized interface connections and module instantiations. The UVM components— agents, environments , and scoreboards  are implemented using  a flexible, parameter -driven architecture , enabling rapid adaptation to new RTL builds. Integrated with a Makefile-based flow, the methodology supports one-command generation of both DUT and testbench. Results show a significant reduction in bring-up time, full reuse of verification components, and successful deployment across multiple DUT variants  with zero manual edits . |
| DVConEU 2025 paper 100 | — This paper shows how MATLAB can support the verification of complex mathematical algorithms in a SystemVerilog-UVM environment. This case study is based on a real project of a RADAR SoC (System On Chip) with a complex DSP (Digital Signal Processor) being embedded in the device. As part of the verification process, a model to generate the expected results for each of the DSP steps had to be implemented. In the first phase of the project, floating- point models to predict the results have been implemented in SystemVerilog. However, the implementation and the debugging of the models required a significant effort. Moreover, the incrementing complexity and the addition of new DSP features made the SystemVerilog models hard to maintain. In the second phase of the project, the algorithm team provided fixed-point MATLAB models matching the RTL implementation of each algorithm. The MATLAB models were reused by the verification team to replace the SystemVerilog floating-point models, allowing the team to focus on the implementation of testcases, checks, and coverage, rather than the implementation of the model . The transition from a floating -point model to a fixed -point model is also particularly important for t his kind of applications, since errors of one LSB could compromise the correctness of the results. The integration of the MATLAB functions was done by using SystemVerilog DPI (Direct Programming Interface), which allowed to easily reuse the MATLAB code fro m the testbench. This not only shortened the verification time and effort, but it also allow ed to reduce to the minimum the error tolerance used in the testbench checks. |
| DVCon Taiwan 2024 1.7 Synopsys Left shifting Testbench Development Using Environment Inversion in UVM | N/A |
| DVCon Taiwan 2024 1.2 AMD Improving UVM test benches using UVM Run time phases | N/A |
| DMS Verification Environment for Gyroscope System | N/A |
| Comprehensive Configurable Ethernet IP verification strategy | N/A |
| Cocotb and PyUVM tests powered with pytest | N/A |
| Automated Flow to Maintaining Consistency in Parallel Design Representations Using Cross Level Verification | N/A |
| An Elegant scoreboard eco system deploying UVM Callbacks Parameterization for Multimedia designs from Imaging perspective Chakravarthi Nanda | N/A |
| AI Pair or Despair Programming Using Aider to build a VIP with UVM SV and PyUVM | N/A |
| Advancing Open Source Verification Enabling Full Randomization in Verilator | N/A |
| ACT with Confidence Formal Verification of Packet Based Designs using Array Centric Tracking | N/A |
| Accelerating Coverage Closure with Reinforcement Learning A Case Study on FSM Verification | N/A |
| Accelerate Verification of Complex Hardware Algorithms using MATLAB based SystemVerilog DPIs | N/A |
| A UVM Testbench for Exploring Design Margins of AnalogMixed Signal Circuits A PCI Express Receiver Detection Circuit Example | N/A |
| A Novel Configurable UVM Architecture To Unlock 1.6T Ethernet Verification Sameh Mahmoud | N/A |
| 92144 | — System-level verification of t oday’s System on a Chip (SoC) is highly challenging, having to consider both hardware and software (program code) stimuli and checking interaction at the same time, while maintaining reusability, scalability and a timely delivery with high quality. Integrating both C tests (containing the software instructions) and the Universal Verification Methodology (UVM) code (containing the rest of the testbench components aimed to verify the hardware) into a single verification testbench is a main approach for SoC level verification. However, this approach brings limitations for the control, reusability and randomization of tests. This paper demonstrates a technique that allows dynamically random generation and driving from an UVM verification environment of CPU instructions for SoC verification, using mainly a black-box approach for the design. The solution accommodates any design with one or more CPU cores, it is flexible for design changes and even for CPU Instruction Set Architecture (ISA) changes . The solution is fully compatible with UVM and non -UVM based verification testbenches. Examples show how this method is implemented in a testbench that automatically adapts and works with any microcontroller design configuration. |
| 91809 | — One of the principal features driving mobile market is the camera specification. Based on the needs of consumer market, there is a demand for camera sensors from the entry level to premium ones. An Image Sensor SOC will have many image processing sub -blocks to capture a high quality image. A CMOS image sensor has an Analog component which captures light, converts it to digital value and feeds it to digital pipeline. There are noises involved in Analog operation and manufacturing defects during fabrication. The digital pipeline has to mitigate these noises and defects for a high quality image output. Finally, the high quality image is sent to the host via Industry standard serial protocol. For a high quality image, advanced image processing IPs and algorithms are developed, resulting in increased design complexity. Additionally, the ever increasing need for higher resolution Image Sensors leads to larger designs. Due to market demand and aggressive schedules, the timelines for Design Verification shrink along with emphasis on quality validation. The key component for verification of any ISP (Image Signal Proces sing) algorithm is scoreboard, where we compare RTL output against golden reference model. A robust, configurable scoreboard architecture is vital to meet the verification quality in a short time frame. This architecture needs to be reusable across all sub-blocks. This paper details the practices followed in scoreboard implementation to achieve high quality verification within the stipulated time. |
| 91176 | — This Paper demonstrates the continuum for multi-Core architecture integrating UPF-based Low Power methodologies and strategies for L1 and L2 Cache transitioning in different modes, like Off, Sleep, Dormant, and Retention in PowerUp/Down sequence within the UVM Low Power Package with in -built ASM routines and incorporates these within low power UVM classes using SystemVerilog and DPI. This addresses the limitation of previous works (referenced) to incorporate multi -Core low-power libraries (which in turn have the classes for SOC environment Devices, Buses and Memory) for low-power strategies. These low power libraries are imported in UVM environment. |
| 90997 | — EM Microelectronic focuses on ultra-low power, low voltage integrated circuits for battery-operated and field-powered applications. It means that from the verification perspective, system-level mixed-signal test-benches with many power-aware checks are usually developed. The main contribution of this paper is a highly reusable UVM test - bench skeleton, which can be adapted step by step to verify the blocks of the system, as well as the whole System on Chip (SoC), including the HW-SW co-verification with firmware. This test-bench is top-level based; its unique feature is the possibility of parallel development of individual blocks and their UVCs by different verification engineers at the same time while sharing only a few common structures where these people cooperate: UVM layering (with translation) of system transactions, a global register model, processing of interrupts and a global UVM event pool. This feature allows a significant acceleration of development and is essential for reuse in future projects. |
| 90555 | - In the last 10 years SystemVerilog and the Universal Verification Methodology (UVM) have enjoyed widespread adoption by the industry, becoming the de facto standards for digital design and verification. Despite their popularity, as of 2023 no open-source simulator supports enough SystemVerilog constructs to allow a complete imple- mentation of UVM, relegating these methodologies to costly closed-source tools. This paper explores using Verilator, a SystemVerilog to SystemC/C++ conversion tool, and Accelera’s UVM-SystemC library to build an industry level open-source verification environment. Finally, a case study is presented in which the proposed framework is used to verify a RISC-V microprocessor, taking full advantage of the features offered by this unique combination of tools and methodologies. |
| 5B3 DVCon India 2024 Final Paper 2224 | — SoC Software bring-up and its validation are usually done at post-silicon stage, which can affect time to market a product, and cost of silicon re-spin. It is also difficult to debug hardware/software design bugs at post-silicon level because of less RTL design visibility. So early software bring-up and validation is very crucial. Therefore, Platform Teams need a high -speed model of the SoC months ahead of silicon availability to begin Software bring -up and application driver development. Because of Less Emulator driverClk frequency software bring -up cannot be done in FPGA based Emulation Verification environment. Therefore, Platform T eams need a high -speed model of the SoC , months ahead of silicon availability to begin Software bring-up and application driver development. To overcome less emulator speed and to accelerate Software development, Hybrid Emulation is developed. Hybrid emulation combines virtual prototyping and hardware emulation. To speed up the hybrid emulation, some components of SoC move from emulator to virtual side. So in hybrid Emulation, one part of SoC design runs at emulator and other part of design run in virtual platform. Virtual prototypes are high performance, System  C models of a particular IP, block or an entire SoC. These are modelling the behavior and inter-block communication at transaction level (TLM), which makes these faster than equivalent cycle-accurate RTL. |
| 5B1 DVCon India 2024 Final Paper 3974 | -The typical design verification process in simulation, reliant on RTL, provides detailed insights into SoC chipsets but may not adequately capture the physical implementation aspects and switching activities. To address this, the industry increasingly turns to Gate Level Simulation, tailored for specific IP and test features, despite its tediousness and high time -consuming nature. Thus, innovation is crucial for efficient SoC design verification, ensuring accuracy and reliability without compromising speed. Gate Level Emulation, executed on physical hardware, offers faster validation times due to higher clock frequencies (in the Mega Hertz range), making it ideal for Gate Level Verification while maintaining reliability. In this paper, we introduce a cutting -edge GLE framework featuring custom Application Specific Integrated Circuit (ASICs) alongside an advance software test bench for design verification and debugging. Our platform is notable for its optimization techniques, which substantially enhance efficiency and benchmark its performance against two complex SoCs. Furthermore, we demonstrate this GLE platform's superiority over traditional GLS by highlighting a remarkable execution time improvement of 55X. Additionally, our test scenarios achieve around 60% greater coverage compared to conventional GLS. This robust GLE platform is implemented on the Cadence Palladium Z2 emulator. |
| 5A3 DVCon India 2024 Final Paper 5557 | N/A |
| 5A2 DVCon India 2024 Final Paper 7316 | -The architecture and design of embedded microcontroller units (MCUs) involve numerous peripherals and use cases, necessitating thorough verification of system IPs to ensure reliability and functionality. This verification is essential for managing enablement, reset, clock, and power functionalities at both the IP and System-on-Chip (SoC) levels. This paper introduces an automated methodology using the Automated PSS Generation Utility (APGU) for verifying System IPs and porting them to SoC architectures. APGU supports Object-Oriented Programming (OOP) principles, enabling modular and reusable test scenarios that are easily adaptable. Additionally, the Portable Stimulus Standard (PSS) facilitates the automatic generation of C-based tests, streamlining the verification process and ensuring consistency. By leveraging portability and automation tools, verification teams can achieve higher efficiency, scalability, and productivity, leading to more robust and reliable MCU designs. I. |
| 4C3 DVCon India 2024 Final Paper 2603 | When it comes to verifying a Superscalar RISC -V processor’s Debug unit, the process involves not only verifying the Debug unit itself but also verifying its interactions with variety of processor configurations in addition to receiving external inputs from outside.  Verification poses several challenges, including configuring the processor with different settings, mixing instructions in the pipeline, and introducing external stimuli like interrupts, debug exceptions, and bus faults. During simulation, the processor is also subjected to random debug requests. Another challenge involves testing a range of instructions in debug mode with various debug control configurations. Correctness checking is crucial, requiring synchronization of asynchronous events with the Instruction Set Simulator and generating expected outcomes for register and memory updates.  The paper acknowledges the profoundness of the methodology published at DVCON US 2023 [1] in addressing these challenges in verifying a Debug unit of a Scalar RISC-V processor. However, when it comes to verifying a Debug Unit of a Superscalar RISC -V processor, it has limitations in halting the processor precisely  at a desired instruction with the “State Save and Restore” technique described in [1] . The paper meticulously examines these limitations and presents “Software Breakpoint” technique in which  the processor architecture guarantees precise halting at the desired instruction as part of the breakpoint implementation, irrespective of whether it is a Scalar or Superscalar processor.  The paper discusses how this technique is versatile and works well for processors with Instruction cache. Th is technique delivers  an average simulation speedup of 11% as compared to the “State Save and Restore” technique. The paper finally implements this technique  for the verification of Superscalar RISC -V processor’s Debug Unit demonstrating 100 percent functional and code coverage. |
| 4B3 DVCon India 2024 Final Paper 3654 | : We are currently existing in an era where terms like Artificial Intelligence(AI) and Machine Learning(ML) have become common household jargon. To cater for  the ever -growing demand for processin g large amounts of data, High Bandwidth Memory(HBM) was introduced as a solution. HBM addresses the demands like High Bandwidth with low latency memory accesses by providing a wider data bus with upto 2 64 bits of DQ, this means that the amount of data being processed per clock cycles is much more resulting in hi gher speed and efficiency. Since HBM memory is a 2.5D memory consisting of memory dies interconnected using through -silicon vias (TSVs), it offers relatively less latency than other traditional interconnection methods and higher packing density. However, this results in an increased amount of IOs opening up a larger margin for external as well as internal factors to affect its normal operation. Here, we are discussing the causes and resultant effects of one such prominent factor, power noise . We will be modelling its effects in the digital domain and simulate the results. Power noise, in of HBM (High Bandwidth Memory), refers to fluctuations or disturbances in the power supply voltage experienc ed by the memory devices, PHY (Physical Layer), and associated components. These fluctuations can occur due to various factors such as rapid changes in current demand, impedance mismatches, parasitic elements in the power delivery network, and electromagnetic interference. The effects of power noise in HBM memory can be significant and can impact the memory system in several ways: 1. Bit Errors and Data Corruption: Power noise can induce fluctuations in the voltage levels of memory cells, leading to bit errors during read or write operations. These errors can corrupt stored data, resulting in data integrity issues. This also leads to increased bit error rate(BER). 2. Timing Violations : Power noise can disrupt the timing margins of memory operations, leading to violations of setup and hold times. When the timing constraints of memory interfaces are violated, it can lead to data setup failures or hold failures, resulting in data corruption or loss. 3. Reduced Operating Margin : Power noise can reduce the operati ng margins of the memory devices, causing them to operate closer to their specified limits. This reduction in margin can make the memory system more susceptible to variations in operating conditions, such as tempera ture fluctuations or voltage droops, potentially compromising its stability and reliability. Figure 1: HBM memory architecture |
| 4B2 DVConIndia2025 Final Paper 6824 | : The escalating complexity of modern IP and SoC designs has given rise to increasingly intricate reference models. These models are critical for replicating design behaviour and guaranteeing correctness, but their development can be a time-consuming and laborious process. Fortunately, Python's simplicity and flexibility make it an ideal choice for crafting these models with greater speed and concision. In this paper, we delve into the various methods for bridging the gap between Python and System Verilog, exploring their respective advantages and disadvantages. By examining the trade-offs between these approaches, we aim to provide a comprehensive understanding of the best practices for efficient verification flows. I. |
| 4B1 DVCon India 2024 Final Paper 8500 | Power is one of the important parameters impacting today’s electronic designs and with increasing complexities in the power structure and complex power domain definitions, the need to minimize dynamic and static power consumption creates verification challenges. For all low power designs, power intent/UPF is a key and generating a clean UPF helps throughout the design flow and reduces the overall turnaround time of projects.  For complex SoCs, due to increase in physical partitions and supply rail restrictions for various partitions, can make UPF generation challenging, i.e. the UPFs that are being generated need to be physical implementation aware.  These generated UPF needs to be validated thoroughly to make sure UPF is clean from syntax, schematics, consistency w.r.t to power state tables and Implementation aspects and if any issues in the UPF constructs then it’s better to be caught earlier at the RTL stage itself so that issues can be fixed sooner and faster without involving implementation phase/cycle. VC LP static low power verification solution includes UPF Consistency Checks, Structural Power/Ground (PG) Checks and Functional checks and Architectural checks.  Traditional use models of VC LP allow to make sure UPF consistency w.r.t power state tables are clean at RTL stage and at Netlist stage, low power cells are correctly inserted that makes design structurally and electrically correct.  There is an increasing demand to catch and verify various Netlist stage low power issues at the RTL stage itself.  This will shift left the static verification of low power issues. Virtual instrumentation-based flow in VC LP shifts left the low power verification by instrumenting the isolation and level shifter cells in the design based  on the UPF strategies which subsequently leads to the more accurate verification of the design and save a lot of verification time at RTL and Netlist. [Key words:   Low Power, RTL, Isolation, Level Shifter UPF, PST] I. |
| 4A3 DVConIndia2025 Final Paper 0314 | - The increasing adoption of multi -die designs in High -Performance Computing (HPC) poses significant verification challenges. These integrated systems combine multiple dies within a single package to function as a unified System-on-Chip (SoC), offering scalability, high bandwidth, and energy efficiency. However, verifying the correctness and functionality of these complex systems is a daunting task. This paper presents a novel approach using Multi -Die Co - Simulation Framework, a parallel computing te chnique that enables efficient verification of complex multi -die systems. Our work demonstrates the effectiveness of this technology in reducing simulation time, improving verification efficiency, and enhancing scalability, making it an ideal solution for next-generation HPC systems. |
| 4A2 DVConIndia2025 Final Paper 5352 | - The BootROM, implemented as a small mask ROM or write -protected flash memory embedded within the processor chip is responsible for executing the first code when system is powered-up or reset. BootROM code is responsible for fetching all software binaries from external devices which includes the BL (Boot Loader), authenticate it and keep it in the system RAM. To enhance system security, the BootROM integrates AES (Advanced Encryption Standard) and SHA (Secure Hash Algorithm) in the chip, checks the loaded BL for security, and provides personalized key management for the chips to prevent from cyber-attack. The ultimate goal of BOOTING is to fetch all BLs from external device and to bring up the OS (Operating System).  In the ever evolving era of chip manufacturing, conventional approaches of achieving functionality, form factor, cost and much demanding power/performance goals pave the way of transitioning to smaller process nodes [1]. However, manifold increase in compute/performance requirement has pushed the monolithic System on Chip (SOC) to sizes that are challenging to fabricate with acceptable yields. Additionally, the diminishing returns of advanced nodes have made it economically impractical to accommodate all o f the required logic, IO and memory for compute intensive applications within the limits of manufacturing equipment [1]. To address these limitations, chip designers are embraced with multi-chip and multi-die designs where larger designs are partitioned into smaller designs often referred as chiplets. These chiplets are then integrated into a single package for multi-die and different packages for multi -chip to achieve the desired form factor and power goals. Additionally, these approaches provide both scalability and flexibility to cater to different market segments and specific needs. Despite clear advantages of Multi -die/Multi-chip systems, numerous novel and distinctive challenges need to be addressed. Key verification challenges include availability of a scalable test bench to realize multi-chip/multi-die simulation, infrastructure limitation, homogeneous(multi-die) and heterogeneous(multi-chip) design and increase in simulation run time with debug complexities. This paper proposes a unique scalable app roach called “DISTRIBUTED SIM” to overcome all mentioned challenges within constrained verification timeframe. DISTRIBUTED SIM provides a distributed simulation approach where each individual die/chiplet has its own simulation and all simulations communicate with each other over socket at a specified time. Preliminary result shows a 10x improvement in test bench development, 20-30 % improvement in simulation time, 15 % improvement in processor usage and 10x reduction in resource requirement over a conventional single wrapper approach of   multi-die/multi-chip simulation. All these promising figures claim DISTRIBUTED SIM as a robust approach towards multi-die/multi-chip verification. I. |
| 4A1 DVConIndia2025 Final Paper 2911 | —The need for complex computing networks are on the ri se and this  calls for more advanced logic incorporated on a smaller form factor.  Size of monolithic SoCs for Generative AI, hyperscalers & enterprise grade data-centre applications is becoming too big for manufacturability . With increasing complexity and number of ga tes, simulation time is also increasing and the adoption of ne xt-gen verification  techniques are on the rise. With the emergence of complex multi-die SoC designs to tackle yield issues on advanced manufacturing nodes, the challenges in verification have increased manifold. This involves integration of pre-verified IPs, sub-systems, and partially verified chiplets using multiple vendor simulator platforms and Verification IPs. Artificial Intelligence (AI) is at the forefront of ASIC breakthroughs. With time to market being a critical factor, adherence to aggressive schedules has become the new normal. Rebuilding these complex verification environments onto a multi-die SoC testbench in the given timelines is extremely challenging. Universal Chiplet Interconnect Express (UCIe) is an open industry standard, multi-protocol, high-bandwidth (up to 64 GT/s per lane), die -to-die (chiplet) interconnect that standardizes inter -die communication on-package. With |
| 3C3 DVConIndia2025 Final Paper 7251 | - This paper sheds light on the innovative formal verification approach to overcom e thorny  challenges of verifying caching and ordering mechanisms within modern SoCs, where design complexity arises from highly interconnected components, diverse data structures, intricate pipeline stages, and sophisticated ordering rules governing request handling. Traditional methodologies including dynamic simulation techniques and testing struggle to expose the full spectrum of scenarios and subtle corner cases required for robust verification. In this paper, we introduce generic and scalable techniques that tackle these challenges, resulting in a holistic end -to-end solution for the formal verification of caching and ordering logic. Our proposed framework, validated through application, uncovered over 100 bugs — including numerous critical and deeply buried corner -case issues—and enabled timely signoff. These results underscore the significance and effectiveness of formal verification in handling the challenges of complex designs and achieving comprehensive verification in advanced SoC architectures. I. |
| 3C1 DVConIndia2025 Final Paper 0076 | N/A |
| 3B2 DVCon India 2023 Final Paper 285 | —UniversalVerificationMethodology(UVM)SequenceLayeringenablesprotocol-independenttestscenariodevelopmentbyaddinganintermediarylayerbetweenthesequenceandsequencerflow. Theadditionallayerallowshigher-levelcoding,enhancescodereusability, andscalability. ThispapershowcasestheapplicationofUVMSequenceLayeringonRALregistersequences,leveragingCadenceVIP, anddemonstratestwousecasesforPCIeandSPIprotocols.Theimplementationof adapterlayersequenceintegrationrequiredminorRALmodelmodifications.WiththeintroductionofthePCIeandSPIadapterlayersequences,finer-granularitywasachievablewithexistingregistersequences,resultinginfasterverificationturnaroundtimewithminimaladditionaleffort. |
| 3A1 DVCon India 2024 Final Paper 3687 | -With the silicon industry moving towardszero post silicon bugs, the significance of extensiveverification at pre-silicon level is paramount.One of the major challenges in pre-Silicon verification is to developa test suite strong enough to cover all the corner cases unearthing the  potential bugs. Depending on thecomplexity of the design, adding complicated  test scenarios manually is tedious and at times difficult to maintain.As the DV cycle continues, more and more UVM sequences are coded and many conditional  paths get added inthe testbench due to inter sequence dependencies. Any further change needs thorough understanding of all thesequences and requires modifications at multiple places. In this paper, we propose a smarter state pattern basedUVM sequence library and methodology that manages all the scenarios efficiently as the states are kept alignedwith the DUT system state. I. IntroductionAny testbenchscenariocanfundamentallybe brokendownintoa stimulus whichtargets the DUTinonestate andoncompletionof activitymovestheDUTtothesameordifferentstate.Manysuchstimuliarecascadedtocome upwithcomplexscenarios.ConventionallyUVMtestbencheshavemultiplestimuli(sequences)whicharerunin specific orders to obtain the required scenario. The problem with this approach is that there is a lot ofinterdependency of sequences and the user has to ensure every test, virtual sequence is following this. For instance,● Clocks sequence can’t be run before power sequence● Data sequences can’t be run before clock and reset sequences● Multiple power up/down dependency due to multiple power/clock domainsManaging these dependencies during the initial phase is still manageable but whennewdependencies needtobeaddeddue tonewscenariocoding, struggle starts todecodethesequencesandusuallyendupbreakingexistingtestcases. The methodologyproposedinthispaperisasmartwayofmanagingthesequencesviaastatepatternbasedsequence library. The design can be logically split into different units or states. The concept of state patterns isleveraged to layer different verification sequences. The states maintainedinthe testbenchhandles the DUTstatesdependencies hasslefree. Any new additions and modifications is just adding the new sequence to the required state. The full test suite canbe visualizedas a combinationof states whereeverystatehasabunchofsequencesassociatedwithit. Eachsequence will have the initial state asoneofthevaliddesignstatesandwillleadtoanothervaliddesignstate.Thetestalgorithmpicksarandomsequencefortheinitialstate.Thissequencecaneitherleadtoadifferent state or loopbacktothesamestateoncompletion.Oncethesequenceiscomplete,algorithmpicksrandomsequence as per the newstate .This loopcontinues ‘n’ number of times tocover 100%crossscenario.Fig.1isthepictorial representation of the different transitions possible for 2 states in the proposed methodology. Figure 1 : Different possible transitions for 2 states in the proposed methodology. The main advantage of this algorithm is that there is no need to code 100s of test cases for multiplescenarios, once the sequences are correctlyassociatedwiththestates,everythingisautomaticallyhandledandcrossscenarios can be achieved very quickly, similar to SVconstraints. The solution can be implemented with just 1sequence in each state and as more and more scenarios are coded, user just need to add the sequence to thecorresponding state sequence library.The proposed methodology involves :● State aware sequence library○ Data structure to store the sequences and the end states.○ It contains different APIs to add/remove/skip required sequences for any valid design state.○ Biasing hooks to modify the selection algorithm on the basis of past runs.● Test○ Top level test, executing the sequence libraries in accordance to state encountered○ Control to replay one particular arc● TB tool○ For complexdesigns,thenumberofpossiblestatestobeverifiedishigh.Forthosescenariosthereis alsoa provisiontoaddsequence andstates inXLSformat. TBtoolwhichisprovidedasapartof this dumps the UVM data structure which can directly be plugged into the setup.● Directed scenarios command line○ Usercanpassthelistofsequencesinorderofexecution.Toptestwillpickonlythegivenarc.Thisis helpful to create deterministic scenarios The whole scheme is analogous tolinkedlist, where eachnodeoflinkedlistisasystemstateofDUT,andhouses multiple sequences. Each sequence executed will give the next state of DUT and so on. Modifying thescenarios is verystraightforward. Like a linkedlist, nodes canbe removed,added,sandwichedwithoutanychangein base code. One added advantage users get is the logical boundary for the checker's implementation. Say after reset, all thesignals shouldbe parkedtoreset value andonce DUTis inpowereddownstate,alloutputswillbeisolated.Thesechecks caneasilybe implementedinthe respective states, the user is requiredtocallthecheckerbeforeexitingthestate. |
| 3008 Harnessing the Power of UVM for AMS Verification with XMODEL | N/A |
| 3.8 DVCon Taiwan 2025 Tutorial 28 ChipAgents.pptx | N/A |
| 3.3 DVCon Taiwan 2025 paper 15 20250830 | N/A |
| 3.2 5hB6qaZecKn7 DVCon Taiwan 2025 paper 16 | N/A |
| 2C2 DVConIndia2025 Final Paper 9003 | — Ensuring robust hardware security in increasingly complex and integrated System-on-Chip (SoC) designs require systematic, formal, and scalable methodologies. This paper presents a category-driven framework that unifies the Confidentiality-Integrity-Availability (CIA) triad with the rigor of formal verification, integrating both Security Path Verification (SPV) and Formal Property Verification (FPV). We detail a ten-category hardware security classification scheme aligned to Common Weakness Enumeration (CWE) and demonstrate how security properties are systematically derived from this taxonomy. By employing CWE-driven property generation, strategic abstraction, relaxed constraint handling, and staged convergence, our methodology delivers comprehensive, reusable, and early-stage security verification. This approach enables efficient identification and mitigation of vulnerabilities, not only for confidentiality, but equally for integrity and availability, shifting hardware security left in the design lifecycle. The presented framework is broadly reusable, extensible, and provides actionable pathways for rigorous hardware security analysis in modern SoC environments. |
| 2B3 DVConIndia25 Final PPT 5338 | N/A |
| 2B3 DVConIndia2025 Final Paper 5338 | - Formal verification is a pivotal methodology for ensuring the accuracy and dependability of complex digital designs. Its capacity to exhaustively and swiftly verify the design-under-test (DUT) offers a distinct advantage over conventional simulation-based methods. As intellectual property (IP) blocks grow in complexity, applying formal verification to these designs becomes increasingly challenging. A primary obstacle for formal verification engineers is achieving convergence, a difficulty that escalates with design intricacy. This paper examines the deployment of formal verification on a complex graphics block the compression controller. We elucidate our strategy, employing diverse abstraction techniques to manage complexity and enhance convergence. By systematically navigating the intricacies of compression logic, we demonstrate the efficacy of formal verification in ensuring design compliance and optimizing the verificati on process. This study highlights the critical role of formal verification in contemporary hardware development and offers insights into refining verification strategies for sophisticated systems. I. |
| 2A2 DVConIndia2025 Final Paper 4137 | - Single-precision fused multiply -add (FMA) units are fundamental building blocks in AI/ML accelerators, offering high arithmetic throughput and reduced rounding errors critical for training and inference workloads. Their increased adoption in performance -driven architecture demands rigorous and complete formal convergence to ensure functional correctness across all corner cases. However, verifying these designs poses significant challenges: FMAs feature deep arithmetic pipelines and extended exponent widths, both of which enlarge the symbolic state space and exacerbate corner-case sensitivity. Traditional verification strategies —such as assume -guarantee reasoning, helper assertions or case-splits often struggle in this context. These methods rely on clean modularity and local reasoning, which break down in FMAs due to tightly coupled Datapath stages, nonlinear interactions between operand paths, and fused rounding logic. This complexity leads to state -space explosion, fragile constraints, and frequent tool timeouts, making convergence infeasible through standard approaches. We demonstrate a targeted proof decomposition methodology that systematically partitions the algorithm into smaller, logically coherent verification units such as pre -alignment, multiplication, normalization, and rounding stages — based on their structural and arithmetic characteristics. A key aspect of our approach is constraint migration, which ensures that high -level design assumptions and properties are seamlessly transferred and preserved throughout the verification process at the RTL level, maintaining consistency and correctness across abstraction layers. By aligning verification sub-problems with the solver strengths of different tools and leveraging constraint migration, we achieved exhaustive convergence in 4 –6 weeks—cutting down the formal closure timeline by over 50% compared to traditional approaches that typically require 12 –14 weeks for similar complexity designs. Applied to an industrial -grade single-precision FMA design with extended exponent width, our methodology demonstrates practical scalability and complete end-to-end signoff viability in real-world Datapath verification scenarios. I. |
| 2.A DVCon Taiwan 2025 Paper 5 | N/A |
| 2.8 DVCon Taiwan 2025 Entropy Coder HLS HLV 20 | N/A |
| 2.7 S3cUxbnoVpHq DVCon Taiwan 2025 paper 12 | 2025/9/9 2 PCI Express Gen6 (PCIe®  6.0/6.1) introduces significant complexity with new features such as FLIT mode, 64GT/s PAM4 signaling, and enhanced protocols, making verification and debug extremely challenging. This paper presents an efficient debug methodology for complex PCIe ®  Gen6 designs using Cadence's PCIe Gen6 Verification IP (VIP). We focus on two key aspects: leveraging the VIP’s built-in protocol checking capabilities and utilizing trace file logs to streamline debug and analysis. The proposed approach embeds Cadence VIP as a smart agent in the testbench to automatically flag specification violations across transaction, data link, and physical layers. When issues arise, a user can utilize detailed trace files generated by the VIP to quickly pinpoint protocol errors and timing mismatches. We will demonstrate how this combination of proactive protocol checks and trace-driven debugging accelerates root-cause analysis in a PCIe Gen6 environment. Our strategy enables faster identification of FLIT format errors and credit exhaustion, reducing debug time by a significant margin. The results show improved verification efficiency, thorough coverage of PCIe 6.0 features, and insights that can be applied to other complex interface protocols. Overall, the methodology benefits teams facing advanced PCIe verification by ensuring robust, compliance-checked designs with a streamlined debug cycle |
| 1C3 DVConIndia2025 Final Paper 2809 | -Clock Tree Network (CTN) is one of the complex design structures in SOC that controls clock supply for all the synchronized logic on the chip.  It comprises numerous clock component instances enabled with dynamic control to establish Dynamic Voltage Frequency Scaling (DVFS) and Hardware Controlled Auto Clock Gating (HWACG) schemes, effectively contributing to overall performance and power optimization [1].  Due to increasing complexity and stringent performance requirements, the verification of the CTN has also become more intricate.  This paper presents a robust verification strategy with Clock Monitor (CLKMON), a custom verification module developed to monitor, analyze and validate dynamic nature of CTN .  “Accelerated Clock Reference Model Generator” (ACRMG), an in -house tool developed to automate CTN analysis [2], is used to integrate  the CLKMON for all the IP clocks .  The paper also presents how CLKMON is integrated with various SOCs having different CTN topology. I. |
| 1C1 DVConIndia2025 Final Paper 7562 | -The  verification  of  Central  Processing  Unit  (CPU)  accessible  registers  in  complex  System-on-Chips  (SoCs) presents a significant challenge, often caught between the powerful abstraction of the Universal Verification Methodology (UVM) Register Abstraction Layer (RAL) and the bare-metal necessity of C-based tests. C-based tests, essential for architectural and boot-path validation, traditionally lack the sophisticated modeling capabilities of UVM RAL, leading to inefficient, brittle tests and a high incidence of false failures from undocumented or complex register behaviors. This paper introduces a novel, automated methodology that systematically bridges this gap. We leverage UVM RAL as the reference source to generate portable, robust, and efficient C-compatible register test collaterals. The proposed flow utilizes a UVM sequence to extract comprehensive register metadata—including waivers and access policies—into an intermediate Comma-Separated Values (CSV) format, which is then synthesized into C data structures. Furthermore, we detail an intelligent register pruning technique that reduced test suite execution time by over 60% in a production environment. This methodology enhances verification accuracy by seamlessly porting IP-level waivers to the SoC context, has directly led to the identification of 35+ critical design defects, and has fundamentally improved regression efficiency and time-to-market. |
| 1B3 DVCon India 2023 Final Paper 2910 | : ................................................................................................................................................ 2  Related Work: ........................................................................................................................................ 3  Layering Component: ............................................................................................................................ 4  Application: ............................................................................................................................................ 5  CHI2AXI Layering Architecture: ............................................................................................................. 6  Flow Diagram: ........................................................................................................................................ 7  Code Snippets: ....................................................................................................................................... 7 o Test Case main phase: ....................................................................................................................... 7 o Top Level env : ................................................................................................................................... 8 o Layering Agent: .................................................................................................................................. 8 o Conversion Sequence: ....................................................................................................................... 9  Results: .................................................................................................................................................. 9 o Initiated CHI Transaction: .................................................................................................................. 9 o Converted AXI transaction:.............................................................................................................. 10  Case Study 2: Phy bypass component ................................................................................................ 10  Conclusions: ......................................................................................................................................... 11 2 Figure 1 Typical system architecture ................................................................................................................. 3 Figure 2 UVM Layering component ................................................................................................................... 4 Figure 3 CHI to AXI Layering Internal Architecture ............................................................................................ 6 Figure 4 CHI to AXI Protocol Conversion Layering Flow .................................................................................... 7 Figure 5 CHI Initiated Sequence Items .............................................................................................................. 9 Figure 6 Converted AXI Interface fields ........................................................................................................... 10 Figure 7 Phy bypass component ...................................................................................................................... 10  Table 1 CHI to AXI control attribute conversion 5 Table 2 CHI to AXI Memory attribute conversion .............................................................................................. 5 Table 3 CHI to AXI ordering attribute conversion .............................................................................................. 5 Table 4 CHI to AXI protection conversion .......................................................................................................... 5 Table 5 CHI to AXI Excl conversion .................................................................................................................... 6   Abstract: Increasing computing needs and maturing technology changes demands change in how various blocks in System on chip (SoC) are connected. The Advanced Micro controller Bus Architecture (AMBA) bus protocols is an example of interconnect specifications from ARM that standardizes on chip communication mechanisms between various functional blocks (or IP) for building high performance SOC designs. Over the years AMBA protocols as well have undergone updates and some new protocols are also introduced on the way. Due to this even if an IP does not have significant logic change, to be compliant with latest SoC architecture, IP’s input or output protocol gets updated. This calls for verification changes which must be done along with design update. Common proposal for this kind of change is to add a bridge to do conversion between two protocols. UVM component layering concept enables conversion of one complicated protocol transaction items into other type of protocol transaction items. With layering component, we can develop efficient reusable verification component to replace RTL block under development which does protocol conversion. This allows left shifting verification framework development effort which can reduces time to market with parallel development. |
| 1B1 DVConIndia2025 Final Paper 3272 | - UVM Virtual Sequences are used to coordinate st imulus activity across multiple design interfaces. The newest virtual sequence technique util izes a sequencer pool, which si mplifies and unifies the executi on of sequences and virtual sequences. The other three virtual sequence techniques described in this paper are (1) the Virtual Sequencers technique, (2) the test_base init_vseq Technique, and (3) storing and Retrieving Sequencer Handles in the UVM Resource Database. This paper summarizes and examin es: (1) the four virtual sequence techniques . (2) Why engineers sh ould use one of the sequencer aggregator techniques. (3) Why the most commonly used Virtual Sequencer technique is no longer recommended. I. |
| 1A3 DVConIndia25 Final PPT 5986 | N/A |
| 1A3 DVConIndia2025 Final Paper 5986 | - UVM Testbenches continue to be both commonly used and dreaded by verification engineers. Enabling more test creation and more test writers all using the standard UVM interfaces will allow an increase in productivity. This paper will extend previous work that presents a simple task based, C callable interface for tests. This paper will demonstrate an actual implementation of task-based tests built on commonly used bus protocols. Task based tests can be called from any compatible "C callable" interface system, including C, C++, SystemC, Python, Rust, PSS, etc. I. |
| 1A2 DVConIndia25 Final PPT 7616 | N/A |
| 1A2 DVConIndia2025 Final Paper 7616 | - As modern SoCs continue to grow in complexity, debugging remains one of the most time-consuming and resource-intensive phases in frontend verification, both at the IP and SoC levels. In large SoC projects,design blocks are a mix of new, reused, and slightly modified IPs. A common challenge arises when DV engineers, particularly those new to a reused block, encounter test failures and must rely on colleagues with prior experience for debug guidance. This dependency often leads to delays, especially when team members are unavailable or distributed across time zones. This paper presents a novel, experience-driven debug assistance tool designed to reduce this dependency and accelerate the debug process. The tool captures and stores error signatures observed, along with their most probable root causes, effectively mimicking the knowledge transfer that typically occurs between experienced and new verification engineers. When a test fails, the tool analyzes the failure pattern and provides likely debug pointers based on historical data, enabling engineers to resolve issues faster and more independently. I. |
| 1A1 DVConIndia25 Final PPT 4877 | N/A |
| 1A1 DVConIndia2025 Final Paper 4877 | Complex System-on-Chip (SoC) designs extensively rely on intricate interrupt mechanisms for event handling. Traditional hardware interrupt verification uses unstandardized, manual, and error-prone methods that are difficult to scale or reuse, especially dealing with complex interrupt aggregation points. This paper introduces a novel, reusable UVM-based framework designed to overcome the aforementioned issues. We present a solution that models interrupts as a hierarchical object tree, featuring a central manager for building these hierarchies and an intelligent monitor that automates runtime source identification, reporting and end-of-simulation checks. The architecture’s key innovation is its clear separation of DUT-specific integration from the generic, scalable, reusable logic. We detail the design, demonstrate its application on a complex Multimedia Processing Unit (MPU) and discuss its benefits for achieving scalable and robust interrupt verification. |
| 1A1 DVCon India 2024 Final Paper 3195 | — The idea of the p aper is to design a generic architecture that acts as a hardware accelerator and comprises the capabilities of image scaling algorithms like Bilinear, Nearest neighbor, and Box Filter. The generic architecture has the capabilities of a DMA controller and conta ins a scaler unit that helps in resizing images, bilinear scaling, noise  & interference reduction, and also provides the best -downscaled images. The scaler unit acts as a hardware accelerator which helps in memory -to-memory t ransfer. The p aper aims towards building the generic architecture by including the Box Filter along with Bilinear and Nearest neighbor so that it supports more data formats and features keeping in mind the area and speed requirement which are the main aspects of any SOC -based design. The architecture supports NPU  (Neural Processing Unit) data format and Image format along with aligning corners true and False. Finally, the design is verified with the help of the UVM infrastructure testbench and creating prop er stimulus to check whether the design is verified properly and working as per the specification. |
| 184 Trustworthiness Evaluation of Deep Learning Accelerators Using UVM based Verification with Error Injection | —Testing the reliability and trustworthiness of high- performance computing (HPC) applications has made Deep Learning Accelerators (DLAs) verification critically important. In this paper, we introduce a hardware verification framework with an error injection methodology based on the Universal Ver- ification Methodology (UVM) for DLAs that is scalable, reusable, and efficient to test the robustness and resilience of Deep Neural Networks (DNNs) running on various DLA designs. Furthermore, the error injection methodology is applicable to simulation and hardware-assisted verification (HA V) platforms for emulation and FPGA prototyping. Our proposed error injection mechanism is evaluated using Nvidia Deep Learning Accelerator (NVDLA), an open-source DLA core. |
| 159 Hard Math – Easy UVM Pragmatic solutions for verifying hardware algorithms using UVM | — This paper presents pragmatic solutions for verifying complex mathematical algorithms implemented in hardware in an efficient and effective manner. Maximizing leverage of a known-answer-test strategy, based on predefined data scenarios combined with design-for-verification modes, we demonstrate how to find and isolate concept and design bugs early in the flow. This approach allows us to postpone detailed integration of complex mathematical models until much later in the RTL development. The solutions presented are based on real project experience with single chip radar sensors for a variety of applications. The verification environments supporting the presented strategies are based on SystemVerilog and the Universal Verification Methodology. |
| 158 UVM Portable Stimulus Synchronized Multi Stream Parallel State Scenario in UVM | —UVM becomes the state of art meth odology in the area of digital verification. At some instant , there were different competing verification methodologies  and languages such as VMM, AVM and  Vera. VMM mainly relies on the power of scenarios to generate TLM stimuli. Then OVM was developed to be open source which permits cooperation across different EDA providers to improve. OVM was supporting both scenarios and sequences but then it was decided to continue with seque nces as the TLM randomization strategy. UVM has been evolved from OVM and inherits the concept of sequence in turn. While the sequence is playing the main role in UVM verification, it lacks power of scenarios in running multiple sequences in complex random  manner. There was an attempt to include scenarios in UVM but it applies one to one mapping, one sequence runs one scenario.  Also, Portable Stimulus Standard (PSS) is a recent C++ based methodology that aims to generate random UVM tests  based on defining set of possible scenarios. However, it still requires another language to define scenarios and tool to generate the  UVM testcases.  In this paper, a novel approach is developed  in UVM to use multi -stream parallel state scenario adding more complexity in SoC  verification to increase test co verage without additional overhead.  Moreover, it can be exploited by PSS to transfer the verification intent easily using the embedded synchronized schemes. |
| 156 Enable Reuse of SystemVerilog Verification IPs in cocotbpyuvm | —This paper presents a novel strategy for enhancing the Python verification ecosystem by integrating established SystemVerilog Verification IPs (SV -VIPs) utilizing the cocotb and pyuvm framework. Gradually gaining recognition within the verification communi ty, Python-based environments are being explored for their potential to become mainstream in future verification processes. This approach taps into the established SystemVerilog ecosystem, enabling effective reuse of SV -VIPs within Python settings. By leve raging the Direct Programming Interface (DPI -C) and the ctypes library, our method ensures seamless integration between Python testbenches and SV -VIPs. This integration not only utilizes Python's simplicity and readability but also fortifies its capacity for handling sophisticated hardware verification tasks. The paper illustrates this methodology with two practical implementations . It shows Python's evolving significance as a powerful and adaptable verification language and bridges the current divides between software flexibility and hardware verification demands. |
| 155 uvm objection – challenges of synchronizing embedded code running on cores and using UVM | —UVM is a commonly used base class library when testbenches are constructed. However, on the SoC, typically there are embedded cores in the design under test which are also used to execute verification code . The completion of the UVM life cycle is done using the UVM objections. On the embedded software side, a similar implicit SoC specific life cycle is running. This SoC life cycle is defined for sure by the various resets, but also by functions like e.g. logic BIST, clock ramp, security – just to name few. Further, as there are typically multiple cores on the SoC and they themselves have their independent life cycles, the embedded cores among themself need to be synchronized. The completion control from the tests running on the em bedded cores in junction with the UVM testbench is topic this paper. (Style: Abstract) |
| 151 A UVM Testbench for Checking the Global Convergence of AnalogMixed Signal Systems An Adaptive Decision Feedback Equalizer Example | — A UVM testbench capable of verifying the global convergence property of an analog/mixed-signal system is presented. For example, a sign-s ign LMS adaptation algorithm for a d ecision-feedback equalizer (DFE) may converge to a false final state depending on the initial state. To detect the existence of such false final states, the testbench launches a sequence of trial runs, each starting from a rando m, unvisited initial state, until all possible states of the system are tried or traversed, or a problematic initial stat e is found. The simulation is run entirely in SystemVerilog by modeling the analog components of the high-speed wireline transceiver using the XMODEL primitives. To generate a sequence of trial runs based on the previous results and evaluate the termination conditions, the testbench utilizes a shared state coverage database and a global UVM event. The experimental results show that the testbench swiftly uncovers the false final states caused by high channel loss or insufficient constraints, and successfully confirms the global convergence of the adaptation loop when no such issues exist. |
| 149 A Novel Approach for HWSW Co Verification Leveraging PSS to Orchestrate UVM and C Tests | —The complexity of System on Chips (SoCs) continues  to grow rapidly. Accordingly, new standards and methodologies are introduced to overcome these veri fication challenges. The Portable Test and Stimulus  Standard (PSS) from Accellera is one of the standard example s used to pursue such challenges. In this paper we will show a methodology to use PSS to orchestrate the process o f HW/SW co-verification by driving UVM and C tests and controlling the interaction between them. |
| 1160 1 | N/A |
| 1160 | -The comprehensive verification of Analo g Mixed Signal (AMS) design often encounters challenges due to misalignment in development schedules between the analog and digital designs. These discrepancies stem from divergent priorities in simulation cycles, particularly the emphasis on simulation accuracy for SPICE compared to functional verification needs of digital RTL. Furthermore, the time -consuming nature of SPICE solver simulations for analog, as opposed to event -based verilog simulations for Digital, results in significant disparities in completion times, impacting development turnaround times. Over the years, various Mixed Signal Verification (MSV) approaches have emerged to address these challenges, seeking to strike a balance between accuracy and performance trade-offs. These trade-offs, often driven by design dependencies such as the high-frequency nature of SPICE or low-power modeling of RTL (UPF) [1], which introduce significant productivity bottlenecks in test plan completion and functional verification sign -off, especially considering the growing presence of Mixed-Signal IPs. Building upon industry proven MSV methodologies, this paper describes a verification methodology tailored to resolve these challenges. This paper outlines the drivers behind this vision and provides detailed implementation insights. It begins with an overview of the design and the legacy verification flow, encompassing analog behavior using system verilog real number modeling (SV -RNM) for digital simulations and DV test cases. Subsequently, it delves into the intricacies of separate GUI-based SPICE simulations, including stimulus VCD vectors and their generation, followed by an exploration of verification gaps exposed by the "digital and analog verification in parallel" approach. The paper proceeds to offer an architectural overview of the new MSV testbench, supporting Digital Mixed Signal (DMS) and Analog Mixed Signal (AMS) Co-simulation with spice netlist, alongside guiding principles, and implementation details. It explains the approach for dynamically switching RNMs and SPICE netlists in regression runs, outlines the randomized nature of DV UVM testbench, and explores the analog response. I. |
| 1141 2 | N/A |
| 1141 1 | -A UVM-compliant testbench is developed to apply benchtop-style directed tests to an on-chip low-dropout CMOS voltage regulator. Eight directed tests verify the regulator's DC and AC response to line and load fluctuations, its programmed operating modes, power-supply rejection, etc. To enhance the effectiveness of the low-level directed tests in reaching unexpected corner cases, we apply high-level UVM randomization techniques to generate a randsequence of transient tests. Sub-cycle timing for stimulus and response is managed by means of the uvm_event_pool. Our goal is to present the UVM testbench mechanisms and coding techniques that proved most effective in verifying the circuit-level functionality of analog/mixed-signal blocks—a topic outside the usual scope of chip-level UVM testbench development. I. |
| 1141 | N/A |
| 1137 1 | N/A |
| 1137 | - In the world of design verification for analog and mixed-signal (AMS) Systems on Chips (SOCs) there are many problems, some of which are now relatively solved.  AMS modeling has converged on Verilog-AMS and SystemVerilog real numbered modeling (SV-RNM), with simulator support available from major electronic design automation (EDA) vendors.  Behavioral model development productivity is supported with tools available from some smaller EDA vendors.  One of the remaining productivity gaps is in testbench automation.  Digital design teams will often have a System-C, transaction level model (TLM) of the digital system under test from which both the RTL and a Universal Verification Methodology (UVM) testbench can be derived, however TLM does not work well as a specification for an analog or mixed-signal system, and UVM is a complicated stretch for the mixed-signal team to adopt.  For smaller digital blocks where C or TLM models exist, or can be developed, as a reference specification, this specification will drive the development of a UVM bench for the block level design.  However, for the equivalent level in the mixed-signal design comprising several circuits working together: an RF synthesizer, radio receiver or transmitter chain, or even an entire radio transceiver with many digital controls but little digital content, no simple and standard way of quickly creating a verification bench exists.  This is where there is a convergence of a lot of activity ─ the system designer, analog lead, creation and update of the top-level schematic, designers need to start working together, and is therefore the place where there is often difficulty and a source for errors.  It is also where circuit simulation is needed and becomes slow and often infeasible.  This is the area that we address.  We propose a standardized testbench architecture based on UVM and show a method to automate the construction of a bench for each design. |
| 1133 1 | N/A |
| 1133 | —When silicon project documentation is incorrect or misunderstood, costly hardware bugs can escape into production.  In the software realm, Agile methodologies such as Behavior-Driven Development (BDD) and the Gherkin language emerged to ensure code behavior matches the documented intent.  Bathtub is a new library written completely in SystemVerilog which enables silicon design and verification teams to realize the benefits of BDD and Gherkin to facilitate collaboration and generate true living documentation—executable specifications that are always accurate, accessible, and up-to-date.  This paper describes BDD, Gherkin, and Bathtub, and shares findings from their use in a silicon project. I. |
| 1131 1 | Abstract —Python, as a multi-paradigm language known for its ease of integration with other languages, has gained significant attention among verification engineers recently. A Python-based verification environment capitalizes on open- source frameworks such as PyUVM providing Python-based UVM 1.2 implementation and PyVSC facilitating constrained randomization and functional coverage. These libraries play a pivotal role in expediting test development and hold promise for reducing setup costs. The goal of this paper is to evaluate the effectiveness of PyUVM verification testbenches across various design IPs, aiming for a comprehensive comparison of their features and performance metrics with the established SystemVerilog-UVM methodology. I. I NTRODUCTION With the continuous increase in complexity of System-on-Chip (SoC) designs, verification is becoming ever more challenging. As a result, the time required for verification experiences a significant upsurge. Additionally, there is a subsequent need to be more productive and efficient with limited manpower. Industry-utilized methodologies like Constrained Random Verification (CRV), Formal Verification (FV), and Metric Driven Verification (MDV) use SystemVerilog as a language construct. It provides numerous features like object-oriented programming and functional coverage, but learning the language has a steep curve especially for the freshers requiring good understanding of designs. Fig. 1 shows SystemVerilog is the most complicated language in comparison with other programming languages, as it has 1315 specification pages and 248 |
| 1131 | N/A |
| 1125 Tree Data Framework for Code Generation Application of Generating UVM Testbench for Complex Designs 1 | N/A |
| 1116 1 | N/A |
| 1116 | - The SystemVerilog[2] UVM[1] implements a class named uvm_objection. An objection is used to guard code that "isn't done yet". For example, an objection can prevent a process from finishing until some other process agrees. uvm_objections are sometimes overused and are always misunderstood. This paper will explain the implementations a bit and share uses and provide some alternative solutions that are easier to understand, simpler to use, and work transparently. I. |
| 1113 A UVM Reactive Testbench for Jitter Tolerance Measurement of High Speed Wireline Receivers 1 | N/A |
| 1112 1 | N/A |
| 1112 | - As the demand for high -performance computing System on Chips (SoCs) increases, the importance of full - chip verification with both analog and digital subsystems cannot be overstated. However, current full -chip verification solutions focus heavily on UVM-driven digital verification testbenches, which are not feasible for analog circuits due to the high simulation run time of SPICE models. In this paper, we discuss the limitations of traditional full-chip verification flows and propose a comprehensiv e verification approach that includes both digital and analog functionality. We also explore two prevailing approaches to the modeling of analog blocks, the analog mixed-signal (AMS) approach and the digital mixed- signal (DMS) approach, and discuss the tra de-offs when using both approaches. Lastly, we propose a behavioral model solution to integrate analog functionality into the UVM -based chip-level verification environment, which can significantly reduce the risk of failure and ensure the complete functionality of the SoC. I. |
| 1103 1 | N/A |
| 1103 | - Interface classes introduced multiple inheritance to System Verilog in 2012. With that a class isn't tied only to it’s base class but can also inherit properties from other classes. Users prior to 2012 had to work their way around as the language did not have Interface classes, which includes the code for current Transaction Level Modeling (TLM) specification in the Universal Verification Methodology (UVM). It lacks compile-time checks for port and interface compatibility and missing implementations. Additionally, it leaks APIs between different interfaces, allowing nonsensical and illegal method calls that are only detectable at run-time. With the |
| 1099 1 | N/A |
| 1099 | -Verification IPs are the building blocks of UVM testbenches, needed for Metric Driven Verification of complex designs. UVM Testbench generators can instantly create a basic UVM testbench template from scratch , but the VIP integration must be done manually. This makes  the testbench development activity difficult and time -consuming. Automating VIP integration is the solution, but this is not straightforward due to the lack of an industry-wide standard to exchange VIP metadata. In this paper, the authors present a non -proprietary VIP metadata template that can enable this automation via a Testbench Generator. This paper will further highlight how, without restricting the creativity of VIP developers, multiple vendor VIP titles have been successfully integrated into the ADI's UVM Testbench generator, with the help of this metadata. This method has enabled the DV engineers to instantly create a ready-to-simulate sophisticated UVM TB from scratch, reducing the efforts from weeks to minutes. I. |
| 1086 UVM SV Feedback Loop The foundation of self improving testbenches 1 | N/A |
| 1086 Dont Go Changing How to Code Immutable UVM Objects | N/A |
| 1086 | – In object-oriented programming (OOP), immutable objects enhance the simplicity, reliability, and performance of software. This paper introduces immutable objects, explains how to create them in SystemVerilog, and addresses challenges in incorporating them within the Universal Verification Methodology (UVM). |
| 1078 Real time synchronization of C model with UVM Testbench | N/A |
| 1078 2 | - In ASIC verification, C-based models are essential for managing complex computations and arithmetic processing, serving as golden reference models. This paper introduces a methodology that leverages DPI-C (Direct Programming Interface for C) to automate the synchronization between C models and UVM (Universal Verification Methodology) testbenches. By integrating DPI-C functions into UVM Register Abstraction Layer (RAL) adapters, we achieve real-time updates and dynamic monitoring of bus traffic, significantly reducing the manual effort typically required by UVM predictors and scoreboards. I. |
| 1077 | - Universal Verification Methodology (UVM) [1] supports various verbosity levels (i.e. UVM_FULL, UVM_NONE) for outputting debug to a transcript or log as configured by the report action. This can be set independently for IDs on a component level. Debug output to a transcript or log can consume a large amount of simulation time and generate large log files which can become difficult to parse. Large log files also add additional storage costs. These issues worsen as the verbosity level is increased, leading to more output. Verbosity is often increased to track specific transactions through the Bus Functional Model (BFM) however this causes debug output for all transactions to be generated. We demonstrate a method that borrows from the tried and tested UVM factory method to easily and dynamically escalate the verbosity of transactions and streams. This allows the verification engineer to focus on only relevant debug, accelerating the time to reach a failure point compared to just simply increasing the test verbosity an d enabling a quicker turnaround time on fixes. The method is invoked at run-time, often avoiding the need to recompile.   I. |
| 1060 | - With the rise of AI, IoT, and automotive safety applications, the demand for reliable mixed-signal verification in Integrated Circuit (IC) design has become critical. Increasing design complexity, particularly at the interface of analog and digital components, often leads to verification gaps and costly respins. This paper introduces the Analog Mixed-Signal Verification (AMSV) utility framework, which enhances system-level testbenches by integrating analog behavioral models within a Universal Veri fication Methodology (UVM) environment. Leveraging SystemVerilog’s object -oriented programming (OOP) capabilities, the AMSV framework enables precise monitoring and control of analog models, allowing for accurate simulation of real -world conditions, includ ing temperature effects, parametric variations, and subsystem failures. Through detailed case studies, we demonstrate the framework’s capacity to address verification needs in advanced applications, significantly improving accuracy and efficiency. Simulati on results validate the framework’s potential to enhance IC reliability, making it particularly relevant to emerging, high-stake domains. I. |
| 1047 1 | N/A |
| 1047 | – This paper introduces a multi -agent Verification IP (VIP) architecture tailored for next -generation, high- speed data transfer protocols. Employing UVM, the architecture addresses the complexity inherent in layered protocol subsystems by decentralizing the verification process across multiple UVM agents. This design enhances granularity in verification, yielding improvements in reusability, controllability, and observability. The use of protocol - configurable agents facilitates dynamic stimulus generation and timely observation, ensuring backward compatibility and reducing development cycles. Key to this architecture is the synchronization mechanism across agents, which allows the handling of multi-protocol traffic, which is crucial for the verification of complex multiplexed protocols such as CXL and PCIe6.0. The |
| 104 Migrating from UVM to UVM AMS | N/A |
| 1019 2 | - As system complexity continues to increase, there is a growing need for more effective verification methods. This work proposes a novel solution to tackle this issue by leveraging machine learning techniques, including K-means++ clustering, to pinpoint the most relevant stimuli from a broad set for a particular design within the UVM environment. By examining the design's features and the effects of various stimuli, the machine learning system will strategically choose and generate the most impactful stimuli, thereby enhancing the efficiency of the verification process. With the deployment of  the machine learning model, improvements in transaction efficiency by 49 percent and a notable decrease in the time needed to reach complete functional coverage were observed. I. |
| 1013 | - In this paper, we will discuss about UVM-based Analog and Mixed -Signal (AMS) Verification performed on a Power Management IC (PMIC) that was designed to power OLED mobile display panels . Earlier the focus of the verification was to perform Digital Mixed-Signal (DMS) Verification using SystemVerilog Real Number Models (SV-RNM) to cover various scenarios of the PMIC, such as basic power up and power down sequence, I2C write and read  transactions, Dynamic Voltage Scaling (DVS), and testing the PMIC’s protection logic a gainst various fault scenarios . In this paper , we present how we performed AMS verification of a subset of those DMS items by re-using the UVM-based verification environment from DMS to AMS. We illustrate some key test scenarios that were verified accurately by running AMS simulations, present different mixed- signal configurations created for different scenarios, and describe how we could unearth critical  design issues with the verification methodology used. We have also mentioned whether we could correlate the simulated data from UVM -AMS verification with the actual silicon data to achieve confidence on the verification performed. |
| 1008 1 | N/A |
| 1008 | The advent of opensource hardware, epitomized by the RISC-V processor, is opening up the avenues for opensource hardware verification tooling and flows. RISCV-DV [1] is an opensource random instruction generator widely used for functional verification of RISC-V cores [2]. This paper expounds on a parallelized RISCV-DV port [3] coded in Embedded UVM (eUVM). Novel techniques for enhancing testbench performance are also explored to affect a multicore UVM testbench implementation that generates millions of constrained-randomized RISC-V instructions in a second (using multicore parallelism), a speedup of over 100x when compared to the original RISCV-DV implementation coded in SystemVerilog (SV) UVM. I. I NTRODUCTION A. The Curious Case of Processor Verification High-end RISC CPU cores encompass complex hardware architectures comprising intricate maneuvers like instruction re- ordering, pipelines, branch prediction, and hyperthreading. Functional verification of such processors requires a significant effort involving generation of 1015 (thousands of millions of millions) constrained-random instructions [4]. The RISCV-DV project, coded in SV/UVM, generates only about 10,000 instructions in a second. At this rate, it takes several thousand machine years just to generate the required number of sequences of randomized RISC-V instructions. B. The Era of High Performance Computing Thanks to Dennard’s Scaling [5], CPU frequency and performance grew exponentially until the year 2005. For verification engineers, Dennard’s Scaling proved to be a boon. During the period of its impact, any increase in complexity of hardware designs (owing to Moore’s Law) was offset by a corresponding increase in CPU frequency and performance, thus affecting proportionately faster simulation runs. Dennard’s Scaling continued to hold its sway until the year 2005. Post that, its impact started to wane. That very year, Herb Sutter wrote a seminal paper titled “The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software” [6]. Henceforth, software developers could not rely on an increase in processor frequency as a means to improve software efficacy. Programmers were now required to code multicore-enabled concurrent programs in order to boost software performance, marking the start of the era of High Performance Computing (HPC). Transistor Count (thousands) Frequency (MHz) Typical Power(Watts) Number of Logical Cores Single Threaded Performance (SpecINT X 10  )3 Data Sourced From: https://github.com/karlrupp/microprocessor-trend-data * * The Era of HPC The Era of Free Lunch Fig. 1: CPU Performance Over the Last 50 Years C. The Elephant in the Room Testbench performance is proving to be the proverbial elephant in the room. Even as modern HDL Simulators enable parallelized multicore simulation of RTL designs [7], little has been done to enable multicore parallelization of testbenches (refer section II-A), forcing verification engineers to adopt pragmatic techniques to enhance testbench efficacy. A distributed stimulus generator architecture that invokes Inter-Process Communication (IPC) for the exchange of generated transactions between multiple simulator instances (running as separate Linux processes) is explored in [8]. A multicore predictor architecture that utilizes a C++ thread pool via Direct Programming Interface (DPI) is realized in [9]. Contemporarily, the emergence of hybrid CPU/FPGA architectures and SoCFPGAs is enabling co-emulation platforms [10], wherein the Design under Test (DuT) gets mapped to an FPGA, but the testbench still continues to run on an HDL simulator. A “transaction-based acceleration” approach that proposes “an untimed hardware verification language domain” for accelerating the testbench for co-emulation has been broached in both [11] and [12]. 2 Incidentally, SV lacks support for Transaction Level Modeling (TLM) [13] at the very fundamental level. An essential tenet of TLM requires temporal decoupling of data from simulation time and events. All integral variables in SV (including the two-state types such as byte, shortint, int, and longint), and the expressions thereof, implicitly hold a value-change event that enables the end-user to write wait expressions (such as wait(a > b) ). As a result, any computational algorithm coded in SV executes an order of magnitude slower than the corresponding code in C/C++ and any other native programming languages. Section IX-I reflects further on the impact of non-native datatypes on the algorithmic performance of SV testbenches. II. T HE STATE OF THE ART A. SystemVerilog/UVM The golden reference RISCV-DV instruction generator implementation is coded in SV. But complex constraint solving and sub-optimal algorithmic implementation (refer section VI for details) limits the speed of its execution to only about ten thousand instructions in a second. Additionally, SV lacks native data-types. Consequently, any interface to an emulation platform requires interfacing with C/C++ via the DPI layer. Reference [14] offers a detailed exposition on how the runtime overhead of data exchange via DPI can have an adverse impact on testbench performance. From the HPC perspective, tool-level support for multicore parallelism is limited to RTL and Gate-Level simulations and requires Design Level Partitioning (DLP) [15]. Figure 2 depicts a typical verification setup with a partitioned RTL design as DuT. Since RTL coding semantics support only static variable scoping, multicore-enabled SV simulators are able to identify static RTL segments that can be simulated concurrently on multiple threads. Note that the exchange of signal data across the partition boundaries happens only by value (not by reference) and only via pins and ports that can be formally identified by the simulation tool. Therefore, it becomes possible for the simulator to introduce appropriate synchronization locks when passing data across the threads. TESTBENCH RTL Fig. 2: Multicore RTL Simulators On the other hand, a testbench is a completely different beast. A testbench is essentially behavioral, enables automatic data scoping, and allows sharing of data by reference across its various components. To this end, enabling parallelism in a testbench requires user-level programming language constructs [16] that facilitate synchronized access to shared data, which the current SV language standard lacks. Note that, though a testbench is always encapsulated in a separate module (or a program block), events corresponding to the testbench are temporally staggered from those pertaining to the DuT in order to avoid race conditions [17]. Temporal staggering of the DuT signal from those of the testbench is generally accomplished by use of the clocking block construct and/or the program block construct. Consequently, tasks related to a testbench are never executed in parallel to the simulation tasks associated with the DuT. As a result, a sequentially executed single threaded testbench becomes a bottleneck in a UVM/RTL simulation. B. Python - CocoTB [18] and PyUVM Despite being proposed as a futuristic language for Verification [19], Python has a huge drawback from the testbench performance perspective. Being an interpreted scripting language, Python is inherently inefficient and has been benchmarked to be fifty-seven times slower in comparison to native programming languages such as C/C++ [20]. 1) Pygen – The Python Port of RISCV-DV : Pygen is a python-based opensource port of RISCV-DV and is hosted as a sub- folder of the main RISCV-DV GitHub repository [1]. Pygen uses PyVSC [21] for solving constraints and is currently marred by poor runtime performance. Additionally, PyVSC does not deploy a Binary Decision Diagram (BDD) solver and relies completely on Satisfiability (SAT) solvers for solving multi-variable constraints. Consequently, the solutions produced by PyVSC tend to have a non-uniform spread compared to SV and eUVM. Pygen currently performs more than a hundred times slower when compared to the SV version of RISCV-DV and generates less than a hundred random RISC-V instructions in a second. C. SystemC and C++ The SystemC port of the UVM library [22] currently relies on the CRA VE library [23] for constrained randomization. CRA VE provides an integrated interface to a set of BDD and SAT solvers. But as of now, it can not handle complex RISCV-DV constraints. Additionally, SystemC and CRA VE do not offer support for multicore parallelism. The CRA VE library uses wrapper template types (crv variable on line 2, in listing 1) to represent ran- dom variables. On account of that, random variables in CRA VE do not retain their native memory footprint, thus severely hindering its efficacy and prowess. Listing 1: Example Randomizable Class in CRA VE 1class myrand: public crv_sequence_item { 2crv_variable<int> x; 3crv_constraint constraint{x() < 32}; 4// .... |
| 1 Siemens Bangalore DVCON Keynote Sept 2025 | N/A |
